{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "926bada6"
      },
      "source": [
        "Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a110dfce"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkOtOq0jDE0c"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "      <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/FunctionGemma/%5BFunctionGemma%5DFinetune_FunctionGemma_270M_for_Mobile_Actions_with_Hugging_Face.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e624ec07"
      },
      "source": [
        "# Fine-tune FunctionGemma 270M for Mobile Actions\n",
        "\n",
        "This notebook fine-tunes FunctionGemma for the task of taking user request to perform mobile actions through the Hugging Face Transformer Reinforcement Learning ([TRL](https://huggingface.co/docs/trl/en/index)) library.\n",
        "\n",
        "When training [FunctionGemma 270M](https://huggingface.co/google/functiongemma-270m-it) on a Google Colab A100 GPU accelerator, this process can take 60 minutes end-to-end. Run each code snippet to:\n",
        "\n",
        "1. Set up the Colab environment\n",
        "2. Prepare a dataset for fine-tuning\n",
        "3. Load and test the base FunctionGemma 270M model\n",
        "4. Fine-tune the model\n",
        "5. Test, evaluate, and save the model for further use\n",
        "6. Convert the checkpoint to `.litertlm` for deployment\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This colab needs the **A100 GPU**. You will need a Colab Pro subscription or Colab Pay to Go with credits.\n",
        "\n",
        "## Set up development environment\n",
        "\n",
        "The first step is to install the necessary libraries using the `pip` package installer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEK9IfKBqQaA"
      },
      "outputs": [],
      "source": [
        "%pip install torch\n",
        "%pip install -U transformers==4.57.1 trl==0.25.1 datasets==4.4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTuW1LPfLXi9"
      },
      "source": [
        "**You may have to restart your session (runtime) to use newly installed libraries.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef3d54b"
      },
      "source": [
        "##Enable Hugging Face permissions\n",
        "\n",
        "To use Gemma models, you'll need to accept the model usage license and create an Access Token:\n",
        "\n",
        "1. **Accept license** on the [model page](http://huggingface.co/google/functiongemma-270m-it).\n",
        "\n",
        "2. **Get a valid [Access Token](https://huggingface.co/settings/tokens) with 'Write' access (very important!)**\n",
        "\n",
        "3. **Create a new Colab secret** in the left toolbar. Specify `HF_TOKEN` as the 'Name', add your unique token as the 'Value', and toggle 'Notebook access' on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6d79c93"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login into Hugging Face Hub\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0eb2e06"
      },
      "source": [
        "## Load the model\n",
        "\n",
        "You can access [FunctionGemma 270M](https://huggingface.co/google/functiongemma-270m-it) from Hugging Face Hub by accepting the license terms. The instruction-tuned version of the model has already been trained on how to follow directions and with fine-tuning, you'll now adapt it to a new task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18069ed2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "gemma_model = \"google/functiongemma-270m-it\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    gemma_model,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\",\n",
        "    dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
        "\n",
        "print(f\"Device: {base_model.device}\")\n",
        "print(f\"DType:  {base_model.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hI4twbrz0xj"
      },
      "source": [
        "Device should print as `cuda` if you're using a GPU runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB0Fo8MRbxxY"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "To fine-tune FunctionGemma, we utilize the [Mobile Actions\n",
        "dataset](https://huggingface.co/datasets/google/mobile-actions), which is\n",
        "publicly available on Hugging Face. Each entry in this dataset provides:\n",
        "\n",
        "*   The set of tools (functions) the model can use:\n",
        "    1. Turn the flashlight on\n",
        "    2. Turn the flashlight off\n",
        "    3. Create a contact in the phone's contact list\n",
        "    4. Send an email\n",
        "    5. Show a location on the map\n",
        "    6. Open the WiFi settings\n",
        "    7. Create a new calendar event\n",
        "*   The system prompt providing the context like current date and time\n",
        "*   The user prompt, like `turn on the flashlight`.\n",
        "*   The expected model response, including the appropriate function calls."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create patch dataset to address specific error patterns\n",
        "\n",
        "Based on the evaluation results, we identified several error patterns:\n",
        "- Missing or incorrect tools (e.g., `open_wifi_settings`, flashlight operations)\n",
        "- Relative date parsing errors (\"this Thursday\", \"next Friday\")\n",
        "- Argument normalization issues (email addresses, map queries, titles)\n",
        "\n",
        "We'll create a small patch dataset (~100 examples) targeting these specific failures and combine it with the original dataset for improved accuracy."
      ],
      "metadata": {
        "id": "9TpSR8XgQJh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "\n",
        "# Create comprehensive patch examples targeting specific error patterns from eval\n",
        "# Covers: WiFi settings, flashlight ops, relative dates, email case, map queries, multi-tool\n",
        "\n",
        "# Extract failing examples from trained_scored DataFrame\n",
        "# failing_examples = trained_scored[trained_scored['correct'] == False].copy()\n",
        "\n",
        "# Target 75-140 examples focusing on specific error patterns\n",
        "# Using 'user' column for user prompts\n",
        "# error_patterns = {\n",
        "#    'wifi': failing_examples[failing_examples['user'].str.contains('wifi|Wi-Fi|Wi-fi|network', case=False, na=False)],\n",
        "#    'flashlight': failing_examples[failing_examples['user'].str.contains('flashlight|torch|light', case=False, na=False)],\n",
        "#    'relative_dates': failing_examples[failing_examples['user'].str.contains('thursday|friday|monday|tuesday|wednesday|saturday|sunday|tomorrow|today|next week', case=False, na=False)],\n",
        "#    'email': failing_examples[failing_examples['user'].str.contains('email|send|compose', case=False, na=False)],\n",
        "#    'map': failing_examples[failing_examples['user'].str.contains('map|location|address|navigate', case=False, na=False)],\n",
        "#    'multi_tool': failing_examples[failing_examples['target_text'].str.contains('><', case=False, na=False)]  # Multiple function calls\n",
        "# }\n",
        "\n",
        "# Build patch examples list - use the format expected by dataset\n",
        "patch_examples = []\n",
        "target_per_pattern = 20  # Aim for ~20 per pattern = 120 total\n",
        "\n",
        "# for pattern_name, pattern_df in error_patterns.items():\n",
        "    # Sample up to target_per_pattern from each error type\n",
        "#    sample_size = min(len(pattern_df), target_per_pattern)\n",
        "#    if sample_size > 0:\n",
        "#        sampled = pattern_df.sample(n=sample_size, random_state=42)\n",
        "#        for idx, row in sampled.iterrows():\n",
        "#            # Get the full example from eval_dataset using the same user prompt\n",
        "#            matching = [ex for ex in eval_dataset if ex['text'].find(row['user']) != -1]\n",
        "#            if matching:\n",
        "#                patch_examples.append({\"text\": matching[0]['text']})\n",
        "\n",
        "# print(f\"Created {len(patch_examples)} patch examples from failing cases\")\n",
        "# print(f\"Breakdown: WiFi={len(error_patterns['wifi'])}, Flashlight={len(error_patterns['flashlight'])}, Dates={len(error_patterns['relative_dates'])}, Email={len(error_patterns['email'])}, Map={len(error_patterns['map'])}, Multi-tool={len(error_patterns['multi_tool'])}\")\n",
        "\n",
        "# Create Dataset from patch examples\n",
        "# patch_ds = Dataset.from_dict({\"text\": [ex[\"text\"] for ex in patch_examples]})\n",
        "# print(f\"\\nPatch dataset size: {len(patch_ds)}\")\n",
        "# print(f\"Patch dataset sample:\")\n",
        "# print(json.dumps(json.loads(patch_ds[0]['text']), indent=2)[:500] + \"...\")"
      ],
      "metadata": {
        "id": "5jtBuxvFQH91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine patch dataset with original dataset\n",
        "# Repeat patch examples 30 times to emphasize these patternspatch_ds_repeated = concatenate_datasets([patch_ds] * 30)\n",
        "# Combine with original dataset\n",
        "# enhanced_dataset = concatenate_datasets([dataset, patch_ds_repeated]).shuffle(seed=42)\n",
        "\n",
        "# print(f\"Original dataset size: {len(dataset)}\")\n",
        "# print(f\"Patch dataset size (repeated 30x): {len(patch_ds_repeated)}\")\n",
        "# print(f\"Enhanced dataset size: {len(enhanced_dataset)}\")\n",
        "# print(f\"\\nPatch data represents {len(patch_ds_repeated)/len(enhanced_dataset)*100:.2f}% of enhanced dataset\")\n",
        "\n",
        "# Update the dataset variable to use the enhanced version\n",
        "# dataset = enhanced_dataset"
      ],
      "metadata": {
        "id": "1KJ3ffsxQd5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: The enhanced dataset now includes targeted patch examples to address specific error patterns observed in the initial evaluation. This approach:\n",
        "\n",
        "- Maintains the original dataset's diversity while emphasizing problematic patterns\n",
        "- Helps the model learn correct behavior for edge cases (flashlight, WiFi settings, relative dates)\n",
        "- Improves multi-tool calling accuracy\n",
        "- Should result in higher evaluation scores (>0.85) in subsequent fine-tuning runs\n",
        "\n",
        "You can expand the patch dataset with additional examples from your specific error analysis or skip this step if running the initial baseline fine-tune."
      ],
      "metadata": {
        "id": "lfi3RrO5QiGg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2AwsZMkRioU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from random import randint\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "data_file = hf_hub_download(repo_id=\"google/mobile-actions\", filename=\"dataset.jsonl\", repo_type=\"dataset\")\n",
        "dataset = load_dataset(\"text\", data_files=data_file, encoding=\"utf-8\")[\"train\"].shuffle()\n",
        "\n",
        "print(f\"\\n\\033[1mHere's an example from your dataset:\\033[0m \\n{json.dumps(json.loads(dataset[randint(0, len(dataset) - 1)]['text']), indent=2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PafivP8u1Gv9"
      },
      "source": [
        "## Process the dataset for training and evaluation\n",
        "\n",
        "Now that you've loaded your data, format the training dataset into [Prompt-completion](https://huggingface.co/docs/trl/main/en/dataset_formats#prompt-completion) format for more efficient training later (`completion_only_loss=True`). This means the model will only learn from the `completion` instead of the `prompt`.\n",
        "\n",
        "- `prompt` for the non-trainable parts\n",
        "- `completion` for the trainable parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWz32s5h074E"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def apply_format(sample):\n",
        "  template_iputs = json.loads(sample['text'])\n",
        "\n",
        "  prompt_and_completion = tokenizer.apply_chat_template(\n",
        "    template_iputs['messages'],\n",
        "    tools=template_iputs['tools'],\n",
        "    tokenize=False,\n",
        "    # add_generation_prompt is False since we don't need model output after all\n",
        "    # messages.\n",
        "    add_generation_prompt=False)\n",
        "\n",
        "  prompt = tokenizer.apply_chat_template(\n",
        "    template_iputs['messages'][:-1],\n",
        "    tools=template_iputs['tools'],\n",
        "    tokenize=False,\n",
        "    # add_generation_prompt is True since we would like to include\n",
        "    # \"<start_of_turn>model\" in the prompt, if needed.\n",
        "    add_generation_prompt=True)\n",
        "\n",
        "  completion = prompt_and_completion[len(prompt):]\n",
        "\n",
        "  return {\n",
        "     \"prompt\": prompt,\n",
        "     \"completion\": completion,\n",
        "     \"split\": template_iputs[\"metadata\"],\n",
        "  }\n",
        "\n",
        "processed_dataset = dataset.map(apply_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWf9Vlfm9tkh"
      },
      "outputs": [],
      "source": [
        "#@title Review the processed dataset\n",
        "\n",
        "print(\"\\033[1mHere's an example from the formatted dataset:\\033[0m\")\n",
        "print(json.dumps(processed_dataset[randint(0, len(processed_dataset) - 1)], indent=2))\n",
        "\n",
        "longest_example = max(processed_dataset, key=lambda example: len(example['prompt'] + example['completion']))\n",
        "longest_example_token_count = len(tokenizer.tokenize(longest_example['prompt'] + longest_example['completion']))\n",
        "\n",
        "print(f\"\\n\\033[1mThe longest example length is {len(longest_example['prompt'] + longest_example['completion'])} with {longest_example_token_count} tokens. We need to set the max_length larger than the token count in SFTConfig below.\\033[0m\")\n",
        "print(json.dumps(longest_example, indent=2))\n",
        "\n",
        "max_token_count = longest_example_token_count + 100\n",
        "print(f\"\\n\\033[1mUsing max_token_count of {max_token_count} (= {longest_example_token_count} + 100) for training below.\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QPMjN_vwQf5"
      },
      "outputs": [],
      "source": [
        "#@title Prepare train and eval dataset.\n",
        "\n",
        "train_dataset = processed_dataset.filter(lambda example: example['split'] == 'train')\n",
        "eval_dataset = processed_dataset.filter(lambda example: example['split'] == 'eval')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3w3b9-O4fDz"
      },
      "source": [
        "## Recommended: Test the base model\n",
        "\n",
        "Now, we have loaded both the base model and the dataset. Let's first check how the base model's ability to respond to  \n",
        "a random sample.\n",
        "\n",
        "Try testing it a few times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S8X9LBKYD3f"
      },
      "outputs": [],
      "source": [
        "#@title Test with a prompt\n",
        "\n",
        "from transformers import pipeline\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "# Create a transformers inference pipeline\n",
        "pipe = pipeline(\"text-generation\", model=gemma_model, tokenizer=tokenizer)\n",
        "\n",
        "user_prompt = \"Schedule a \\\"team meeting\\\" tomorrow at 4pm.\"  #@param {type:\"string\"}\n",
        "messages = [\n",
        "    {\"role\": \"developer\", \"content\": \"Current date and time given in YYYY-MM-DDTHH:MM:SS format: 2024-11-15T05:59:00. You are a model that can do function calling with the following functions\"},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "]\n",
        "\n",
        "# Reuse the tools from the sample\n",
        "tools = json.loads(dataset[0]['text'])['tools']\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tools=tools,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True)\n",
        "\n",
        "print(f\"\\n\\033[1mPrompt:\\033[0m {user_prompt}\")\n",
        "output = pipe(prompt, max_new_tokens=max_token_count)\n",
        "model_output = output[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "print(f\"\\n\\033[1mBase model output:\\033[0m {model_output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNfNON0yBJlg"
      },
      "source": [
        "Note that how the base model is unable to successfully call the `create_calendar_event` function for this prompt.\n",
        "\n",
        "Now, we will pick a sample from the training dataset and see how it performs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvgTeVQCAcxe"
      },
      "source": [
        "## Test with training dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8L0_INJyUok"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "# Create a transformers inference pipeline\n",
        "pipe = pipeline(\"text-generation\", model=gemma_model, tokenizer=tokenizer)\n",
        "\n",
        "# Select a random sample from the test dataset\n",
        "rand_idx = randint(0, len(train_dataset) - 1)\n",
        "test_sample = train_dataset[rand_idx]\n",
        "\n",
        "input_prompt = test_sample['prompt']\n",
        "expected_output = test_sample['completion']\n",
        "\n",
        "# Generate the output\n",
        "output = pipe(input_prompt, max_new_tokens=max_token_count, skip_special_tokens=False)\n",
        "actual_output = output[0]['generated_text'][len(input_prompt):].strip()\n",
        "\n",
        "print(f\"\\n\\033[1mInput prompt\\033[0m   : {input_prompt}\")\n",
        "print(f\"\\n\\033[1mExpected output\\033[0m: {expected_output}\")\n",
        "print(f\"\\n\\033[1mActual output\\033[0m  : {actual_output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph26HDJgua3W"
      },
      "source": [
        "The base model output may not meet your expectationsâ€”and that's okay!\n",
        "\n",
        "FunctionGemma 270M was designed for task specialization, which means it can improve performance for specific tasks when trained with representative examples. Let's fine-tune the model for more reliable outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbd9fc1b"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "Hugging Face [TRL](https://huggingface.co/docs/trl/index) provides tools for training and fine-tuning LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BJFoOdL0y8w"
      },
      "source": [
        "### Configure the tuning job\n",
        "Define the training configuration for the FunctionGemma base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiIj1ADc-exw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from trl import SFTConfig\n",
        "\n",
        "output_dir = \"/content/mobile-actions-functiongemma\"  # Where to save your fine-tuned checkpoints\n",
        "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=output_dir,                            # Directory to save adapters\n",
        "    num_train_epochs=4,                               # Number of training epochs\n",
        "    per_device_train_batch_size=8,                    # Batch size per device during training\n",
        "    gradient_accumulation_steps=4,                    # Gradient accumulation during training\n",
        "    logging_strategy=\"steps\",                         # Log every steps\n",
        "    eval_strategy=\"steps\",                            # Evaluate loss metrics based on steps\n",
        "    eval_steps=50,                                    # Evaluate loss metrics every 50 steps\n",
        "    logging_steps=50,                                 # Log loss metrics every 50 steps\n",
        "    save_strategy=\"steps\",                            # Save checkpoint every epoch\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",                       # Cosine scheduler is often better for full FT\n",
        "    max_length=max_token_count,                       # Max sequence length for model and packing of the dataset\n",
        "    gradient_checkpointing=True,                      # Use gradient checkpointing to save memory\n",
        "    packing=False,                                    # Groups multiple samples in the dataset into a single sequence\n",
        "    optim=\"adamw_torch_fused\",                        # Use fused adamw optimizer\n",
        "    bf16=True,                                        # Use bf16 for mixed precision training\n",
        "    completion_only_loss=True,                        # Train on completion only to improve quality\n",
        "    report_to=\"none\",                                 # No reporting.\n",
        "    load_best_model_at_end=True,                      # Load the best model at the end of training\n",
        "    metric_for_best_model=\"eval_loss\",                     # Use loss to evaluate best model\n",
        "    greater_is_better=False,                          # Lower loss is better\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    gemma_model,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.bfloat16,\n",
        "    attn_implementation='eager')\n",
        "\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"Training configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd88e798"
      },
      "source": [
        "### Start training\n",
        "\n",
        "`SFTTrainer` tokenizes the datasets and trains the base model using the hyperparameters from the previous step.\n",
        "\n",
        "The training time varies based on a range of factors, such as the size of your dataset or number of epochs. Using a A100 GPU, this takes about 8 minutes for 1 epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqacJNeU9v7b"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "# Train and save the fine-tuned model\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Fine-tuned model saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDvGlb5xO34z"
      },
      "source": [
        "The weights for each training checkpoint (epoch) will be saved in your temporary Colab session storage. Now, you can evaluate the training and validation loss metrics to choose which checkpoint to for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xll8zZ3_u8Mt"
      },
      "source": [
        "### Plot training results\n",
        "To evaluate the model, you can plot the training and validation losses using Matplotlib to visualize these metrics over training steps or epochs. This helps monitor the training process and make informed decisions about which hyperparameters to adjust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPN-DTopaUIy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Access the log history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Extract training / validation loss\n",
        "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
        "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
        "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
        "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
        "\n",
        "# Plot the training loss\n",
        "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
        "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss per Epoch\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf86e31d"
      },
      "source": [
        "### Test the fine-tuned model\n",
        "\n",
        "Let's compare your fine-tuned model performance against the base model! Test a few inputs by updating `user_prompt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28R3pRN_hai7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Create Transformers inference pipeline\n",
        "trained_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "pipe = pipeline(\"text-generation\", model=trained_model, tokenizer=tokenizer)\n",
        "pipe_base = pipeline(\"text-generation\", model=gemma_model, device_map=\"auto\")\n",
        "\n",
        "# Test a prompt\n",
        "user_prompt = \"schedule a meeting for 4pm tomorrow\"  #@param {type:\"string\"}\n",
        "messages = [\n",
        "    {\"role\": \"developer\", \"content\": \"Current date and time given in YYYY-MM-DDTHH:MM:SS format: 2024-11-15T05:59:00. You are a model that can do function calling with the following functions\"},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "]\n",
        "\n",
        "# Reuse the tools from the sample\n",
        "tools = json.loads(dataset[0]['text'])['tools']\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tools=tools,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True)\n",
        "\n",
        "print(f\"\\n\\033[1mPrompt:\\033[0m {prompt}\")\n",
        "output = pipe(prompt, max_new_tokens=max_token_count)\n",
        "output_base = pipe_base(prompt, max_new_tokens=max_token_count)\n",
        "model_output = output[0]['generated_text'][len(prompt):].strip()\n",
        "model_output_base = output_base[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "print(f\"\\n\\033[1mFine-tuned model output:\\033[0m {model_output}\")\n",
        "\n",
        "print(f\"\\n\\033[1mBase model output:\\033[0m       {model_output_base}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmpAI15UeGcs"
      },
      "source": [
        "### Evaluate the fine-tuned model\n",
        "\n",
        "Evaluating a fine-tuned model is essential to ensure that the process actually improved the model's performance without introducing new issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6HmcMawflSP"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions for evaluation\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def extract_function_call(model_output):\n",
        "    \"\"\"\n",
        "    Parses a string containing specific function call markers and returns\n",
        "    a list of function call objects. Here is an example of the obejct:\n",
        "\n",
        "    ```\n",
        "    <start_function_call>call:open_map{query:<escape>San Francisco<escape>}<end_function_call>\n",
        "    ```\n",
        "\n",
        "    Args:\n",
        "        model_output (str): The model output string.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries representing the function calls.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Pattern to extract the full content of a single function call\n",
        "    # Flags: DOTALL allows matching across newlines if necessary\n",
        "    call_pattern = r\"<start_function_call>(.*?)<end_function_call>\"\n",
        "    raw_calls = re.findall(call_pattern, model_output, re.DOTALL)\n",
        "\n",
        "    for raw_call in raw_calls:\n",
        "        # Check if the content starts with 'call:'\n",
        "        if not raw_call.strip().startswith(\"call:\"):\n",
        "            continue\n",
        "\n",
        "        # Extract function name\n",
        "        # Expected format: call:func_name{...}\n",
        "        try:\n",
        "            # Split only on the first brace to separate name and args\n",
        "            pre_brace, args_segment = raw_call.split(\"{\", 1)\n",
        "\n",
        "            function_name = pre_brace.replace(\"call:\", \"\").strip()\n",
        "\n",
        "            # Remove the trailing closing brace '}'\n",
        "            args_content = args_segment.strip()\n",
        "            if args_content.endswith(\"}\"):\n",
        "                args_content = args_content[:-1]\n",
        "\n",
        "            arguments = {}\n",
        "\n",
        "            # Pattern to extract arguments\n",
        "            # Looks for: key:<escape>value<escape>\n",
        "            # The key pattern [^:,]* ensures we don't accidentally eat previous commas\n",
        "            arg_pattern = r\"(?P<key>[^:,]*?):<escape>(?P<value>.*?)<escape>\"\n",
        "\n",
        "            arg_matches = re.finditer(arg_pattern, args_content, re.DOTALL)\n",
        "\n",
        "            for match in arg_matches:\n",
        "                key = match.group(\"key\").strip()\n",
        "                value = match.group(\"value\")\n",
        "                arguments[key] = value\n",
        "\n",
        "            results.append({\n",
        "                \"function\": {\n",
        "                    \"name\": function_name,\n",
        "                    \"arguments\": arguments\n",
        "                }\n",
        "            })\n",
        "\n",
        "        except ValueError:\n",
        "            # Handles cases where syntax might be malformed (e.g., missing '{')\n",
        "            continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def extract_text(model_output):\n",
        "    \"\"\"\n",
        "    Extracts text content and removing the <end_of_turn> marker.\n",
        "\n",
        "    Args:\n",
        "        model_output (str): The model output string.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    if not model_output or model_output.startswith(\"<start_function_call>\"):\n",
        "        return None\n",
        "    return model_output.replace(\"<end_of_turn>\", \"\").strip()\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "def get_eval_logs(dataset, pipe):\n",
        "  batch_size = 1\n",
        "  logs = []\n",
        "  # Select a random sample from the test dataset\n",
        "  for i, output in enumerate(pipe(KeyDataset(dataset, \"prompt\"), batch_size=batch_size)):\n",
        "    orig_data = dataset[i]['text']\n",
        "    messages = json.loads(orig_data)['messages']\n",
        "    user_message = messages[1]\n",
        "    assistant_first_message = messages[2]\n",
        "    input_prompt = dataset[i]['prompt']\n",
        "    # Generate the output\n",
        "    model_output_only = output[0]['generated_text'][len(input_prompt):].strip()\n",
        "\n",
        "    logs.append(\n",
        "        {\n",
        "            # The original user prompt/query.\n",
        "            \"user\": user_message['content'],\n",
        "\n",
        "            # List of ground truth function call objects.\n",
        "            \"target_fc\": assistant_first_message.get('tool_calls', []),\n",
        "\n",
        "            # Ground truth text response.\n",
        "            \"target_text\": assistant_first_message.get('content'),\n",
        "\n",
        "            # List of model-generated function call objects.\n",
        "            \"output_fc\": extract_function_call(model_output_only),\n",
        "\n",
        "            # Model-generated text response.\n",
        "            \"output_text\": extract_text(model_output_only),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if (i + 1) % batch_size == 0:\n",
        "      print(f\"Eval process: {(i + 1) * 100.0 / len(dataset):.2f}%\")\n",
        "  return logs\n",
        "\n",
        "def get_scored_data_frame(dataset, pipe):\n",
        "  logs = get_eval_logs(dataset, pipe)\n",
        "  logs_df = pd.DataFrame.from_records(logs)\n",
        "\n",
        "  scored = pd.DataFrame()\n",
        "  scored['user'] = logs_df['user']\n",
        "  scored['target_names'] = logs_df['target_fc'].apply(lambda x: [fc['function']['name'] for fc in x])\n",
        "  scored['output_names'] = logs_df['output_fc'].apply(lambda x: [fc['function']['name'] for fc in x])\n",
        "  scored[\"target_arguments\"] = logs_df['target_fc'].apply(lambda x: [dict(sorted(fc['function']['arguments'].items())) for fc in x])\n",
        "  scored[\"output_arguments\"] = logs_df['output_fc'].apply(lambda x: [dict(sorted(fc['function']['arguments'].items())) for fc in x])\n",
        "  scored['target_text'] = logs_df['target_text']\n",
        "  scored['output_text'] = logs_df['output_text']\n",
        "  scored[\"correct_names\"] = scored[\"target_names\"] == scored[\"output_names\"]\n",
        "  scored[\"correct_arguments\"] = scored[\"target_arguments\"] == scored[\"output_arguments\"]\n",
        "  scored[\"correct\"] = scored[\"correct_names\"] & scored[\"correct_arguments\"]\n",
        "\n",
        "  return scored\n",
        "\n",
        "def review(scored):\n",
        "  scored[\"incorrect_names\"] = scored[\"target_names\"] != scored[\"output_names\"]\n",
        "  scored[\"incorrect_arguments\"] = scored[\"target_arguments\"] != scored[\"output_arguments\"]\n",
        "  scored[\"incorrect\"] = scored[\"incorrect_names\"] | scored[\"incorrect_arguments\"]\n",
        "\n",
        "  for index, row in scored[scored[\"incorrect\"]].iterrows():\n",
        "    print(f\"\\033[1mSample #{index} prompt  \\033[0m: {row[\"user\"]}\")\n",
        "    print(f\"\\033[1mSample #{index} expected\\033[0m: {row[\"target_names\"]}, {row[\"target_arguments\"]}\")\n",
        "    print(f\"\\033[1mSample #{index} actual  \\033[0m: {row[\"output_names\"]}, {row[\"output_arguments\"]}\")\n",
        "    print(\"---------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eGq4ZsjjoTl"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate the base model\n",
        "\n",
        "base_scored = get_scored_data_frame(\n",
        "    eval_dataset,\n",
        "    pipeline(\"text-generation\", model=gemma_model, device_map=\"auto\", temperature = 0.001),\n",
        ")\n",
        "\n",
        "base_scored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocOK88aEG6R6"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate the fine-tuned model\n",
        "\n",
        "from transformers import pipeline\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "# Create a transformers inference pipeline\n",
        "trained_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "trained_scored = get_scored_data_frame(\n",
        "    eval_dataset,\n",
        "    pipeline(\"text-generation\", model=trained_model, tokenizer=tokenizer, temperature = 0.001)\n",
        ")\n",
        "\n",
        "trained_scored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMY1P8Rtyopt"
      },
      "outputs": [],
      "source": [
        "#@title Compare the score of the base and fine-tuned models\n",
        "\n",
        "# Optional: save the score in json file\n",
        "trained_scored.to_json('scored_df_20251215_trained.json')\n",
        "base_scored.to_json('scored_df_20251215_base.json')\n",
        "\n",
        "print(f\"\\033[1mBase model score\\033[0m       : {base_scored[\"correct\"].mean()}\")\n",
        "print(f\"\\033[1mFine-tuned model score\\033[0m : {trained_scored[\"correct\"].mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f-3QvCShs5a"
      },
      "source": [
        "## Review what are not matching\n",
        "\n",
        "The score about tell how much eval data get exactly correct function calls. It gives the lower bound of correctness. Some output might not match the eval date but still acceptable. For example, the `show_map` function call below:\n",
        "\n",
        "* show_map:{'query': 'Maison Marulaz, BesanÃ§on, France'}\n",
        "* show_map:{'query': 'Maison Marulaz in BesanÃ§on, France'}\n",
        "\n",
        "Let's take a look at the eval date without exact match. Many of them could also be acceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3-Iwa09zC_K"
      },
      "outputs": [],
      "source": [
        "review(trained_scored)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86qPcFbHH_kh"
      },
      "source": [
        "Does the model call the function you'd expect?\n",
        "\n",
        "If you're not getting the results you want, you can try [using different hyperparameters](#scrollTo=-BJFoOdL0y8w) to train the model, or updating your training dataset to contain more representative examples.\n",
        "\n",
        "Once you're happy with the results, you can save your model to Hugging Face Hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H12D9g4X_peV"
      },
      "source": [
        "## Save your model and upload to Hugging Face Hub\n",
        "**You now have a customized FunctionGemma 270M model! ðŸŽ‰**\n",
        "\n",
        "Upload it to a repository on Hugging Face Hub so you easily share your model or access it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbeyDcpwi4IB"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import ModelCard, ModelCardData, whoami\n",
        "\n",
        "trained_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "#@markdown Name your model\n",
        "model_name = \"mobile-actions_jprtr\"    #@param {type:\"string\"}\n",
        "\n",
        "username = whoami()['name']\n",
        "hf_repo_id = f\"{username}/functiongemma-270m-it-{model_name}\"\n",
        "\n",
        "repo_url = trained_model.push_to_hub(hf_repo_id, create_repo=True, commit_message=\"Upload model\")\n",
        "tokenizer.push_to_hub(hf_repo_id)\n",
        "\n",
        "card_content = f\"\"\"\n",
        "---\n",
        "base_model: {gemma_model}\n",
        "tags:\n",
        "- function-calling\n",
        "- mobile-actions\n",
        "- gemma\n",
        "---\n",
        "A fine-tuned model based on `{gemma_model}`.\"\"\"\n",
        "card = ModelCard(card_content)\n",
        "\n",
        "card.push_to_hub(hf_repo_id)\n",
        "\n",
        "print(f\"Uploaded to {repo_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePMIkvtMT24J"
      },
      "source": [
        "## Conversion to .litertlm for on-device deployment\n",
        "\n",
        "The first step is to install the necessary libraries using the `pip` package installer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No47TBwEjroB"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install ai-edge-torch-nightly --force-reinstall\n",
        "!pip install ai-edge-litert-nightly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DimeKTEw7rbE"
      },
      "source": [
        "### Build the .litertlm from the fine-tuned model\n",
        "\n",
        "After running the script below, you can find the converted model in located in `/content/litertlm/mobile-actions_q8_ekv1024.litertlm` in the colab environment. Copy it to a persistent storage like Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1Sl__hlUWNP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from ai_edge_torch.generative.examples.gemma3 import gemma3\n",
        "from ai_edge_torch.generative.utilities import converter\n",
        "from ai_edge_torch.generative.utilities.export_config import ExportConfig\n",
        "from ai_edge_torch.generative.layers import kv_cache\n",
        "\n",
        "# Metadata for FunctionGemma\n",
        "llm_metadata = r\"\"\"start_token: {\n",
        "    token_ids: {\n",
        "        ids: [ 2 ]\n",
        "    }\n",
        "}\n",
        "stop_tokens: {\n",
        "    token_str: \"<end_of_turn>\"\n",
        "}\n",
        "stop_tokens: {\n",
        "    token_str: \"<start_function_response>\"\n",
        "}\n",
        "llm_model_type: {\n",
        "    function_gemma: {}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "checkpoint_dir = \"/content/mobile-actions-functiongemma\"\n",
        "\n",
        "litertlm_output_dir = '/content/litertlm'\n",
        "os.makedirs(litertlm_output_dir, exist_ok=True)\n",
        "\n",
        "# Create the LLM metadata file\n",
        "metadata_path = os.path.join(litertlm_output_dir, 'base_llm_metadata.textproto')\n",
        "with open(metadata_path, 'w') as f:\n",
        "  f.write(llm_metadata)\n",
        "\n",
        "# Import the weights and build the PyTorch model\n",
        "pytorch_model = gemma3.build_model_270m(checkpoint_dir)\n",
        "\n",
        "# Setup the export configurations and parameters for text generation models.\n",
        "export_config = ExportConfig()\n",
        "export_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED\n",
        "export_config.mask_as_input = True\n",
        "\n",
        "# Convert to LiteRT-LM Format\n",
        "converter.convert_to_litert(\n",
        "    pytorch_model,\n",
        "    output_path=litertlm_output_dir,\n",
        "    output_name_prefix=\"mobile-actions\",\n",
        "    prefill_seq_len=256,\n",
        "    kv_cache_max_len=1024,\n",
        "    quantize=\"dynamic_int8\",\n",
        "    export_config=export_config,\n",
        "    tokenizer_model_path=os.path.join(checkpoint_dir, 'tokenizer.model'),\n",
        "    base_llm_metadata_path=metadata_path,\n",
        "    output_format=\"litertlm\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix JAXlib dependency for .litertlm conversion\n",
        "!pip uninstall jax jaxlib -y -q\n",
        "!pip install jaxlib==0.4.23 jax[cpu]==0.4.23 -q"
      ],
      "metadata": {
        "id": "jSjwUpgnchts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtnMtgeLbDsJ"
      },
      "source": [
        "### Save the `.litertlm` on Google Drive\n",
        "\n",
        "To deploy the converted model on [Google AI Edge Gallery](https://play.google.com/store/apps/details?id=com.google.ai.edge.gallery), we can first store the model on Google Drive. On the Gallery app, import the model from Google Drive later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otpmb9ZkZPpv"
      },
      "outputs": [],
      "source": [
        "#@title Mounting Google Drive on the colab environment\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIa4FHNebNFb"
      },
      "outputs": [],
      "source": [
        "#@title Save the `.litertlm` on Google Drive\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/mobile-actions/\n",
        "!cp /content/litertlm/mobile-actions_q8_ekv1024.litertlm /content/drive/MyDrive/mobile-actions/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8ff452"
      },
      "source": [
        "## Summary and next steps\n",
        "\n",
        "Congratulations! You have completed the first part of [Fine-tune FunctionGemma 270M for Mobile Actions](https://ai.google.dev/gemma/docs/mobile-actions).\n",
        "\n",
        "You have successfully finetuned the FunctionGemma 270M with the Mobile Actions dataset and converted it to `.litertlm` format."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[FunctionGemma]Finetune_FunctionGemma_270M_for_Mobile_Actions_with_Hugging_Face.ipynb",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}