{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "926bada6"
      },
      "source": [
        "Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "cellView": "form",
        "id": "a110dfce"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkOtOq0jDE0c"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "      <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/FunctionGemma/%5BFunctionGemma%5DFinetune_FunctionGemma_270M_for_Mobile_Actions_with_Hugging_Face.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e624ec07"
      },
      "source": [
        "# Fine-tune FunctionGemma 270M for Mobile Actions\n",
        "\n",
        "This notebook fine-tunes FunctionGemma for the task of taking user request to perform mobile actions through the Hugging Face Transformer Reinforcement Learning ([TRL](https://huggingface.co/docs/trl/en/index)) library.\n",
        "\n",
        "When training [FunctionGemma 270M](https://huggingface.co/google/functiongemma-270m-it) on a Google Colab A100 GPU accelerator, this process can take 60 minutes end-to-end. Run each code snippet to:\n",
        "\n",
        "1. Set up the Colab environment\n",
        "2. Prepare a dataset for fine-tuning\n",
        "3. Load and test the base FunctionGemma 270M model\n",
        "4. Fine-tune the model\n",
        "5. Test, evaluate, and save the model for further use\n",
        "6. Convert the checkpoint to `.litertlm` for deployment\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This colab needs the **A100 GPU**. You will need a Colab Pro subscription or Colab Pay to Go with credits.\n",
        "\n",
        "## Set up development environment\n",
        "\n",
        "The first step is to install the necessary libraries using the `pip` package installer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BEK9IfKBqQaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46258f81-4474-4a88-ffb7-9ad1c1d14c0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: transformers==4.57.1 in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: trl==0.25.1 in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: datasets==4.4.1 in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (3.20.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (2.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.25.1) (1.12.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.4.1) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.4.1) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==4.4.1) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.4.1) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.4.1) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets==4.4.1) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (2025.10.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl==0.25.1) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl==0.25.1) (2.9.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.4.1) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.4.1) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.4.1) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.4.1) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets==4.4.1) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1) (2.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.4.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.4.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.4.1) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.4.1) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.4.1) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl==0.25.1) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch\n",
        "%pip install -U transformers==4.57.1 trl==0.25.1 datasets==4.4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTuW1LPfLXi9"
      },
      "source": [
        "**You may have to restart your session (runtime) to use newly installed libraries.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef3d54b"
      },
      "source": [
        "##Enable Hugging Face permissions\n",
        "\n",
        "To use Gemma models, you'll need to accept the model usage license and create an Access Token:\n",
        "\n",
        "1. **Accept license** on the [model page](http://huggingface.co/google/functiongemma-270m-it).\n",
        "\n",
        "2. **Get a valid [Access Token](https://huggingface.co/settings/tokens) with 'Write' access (very important!)**\n",
        "\n",
        "3. **Create a new Colab secret** in the left toolbar. Specify `HF_TOKEN` as the 'Name', add your unique token as the 'Value', and toggle 'Notebook access' on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "b6d79c93"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login into Hugging Face Hub\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0eb2e06"
      },
      "source": [
        "## Load the model\n",
        "\n",
        "You can access [FunctionGemma 270M](https://huggingface.co/google/functiongemma-270m-it) from Hugging Face Hub by accepting the license terms. The instruction-tuned version of the model has already been trained on how to follow directions and with fine-tuning, you'll now adapt it to a new task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "18069ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5feb6696-bf13-4971-aabc-a404c650e11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.4.0 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-944602727.py\", line 4, in <cell line: 0>\n",
            "    base_model = AutoModelForCausalLM.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\", line 601, in from_pretrained\n",
            "    model_class = _get_model_class(config, cls._model_mapping)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\", line 394, in _get_model_class\n",
            "    supported_models = model_mapping[type(config)]\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\", line 807, in __getitem__\n",
            "    return self._load_attr_from_module(model_type, model_name)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\", line 821, in _load_attr_from_module\n",
            "    return getattribute_from_module(self._modules[module_name], attr)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\", line 733, in getattribute_from_module\n",
            "    if hasattr(module, attr):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2317, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 2345, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/modeling_gemma3.py\", line 36, in <module>\n",
            "    from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 28, in <module>\n",
            "    from .processing_utils import Unpack\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\", line 39, in <module>\n",
            "    from .video_utils import VideoInput, VideoMetadata\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/video_utils.py\", line 28, in <module>\n",
            "    from .image_transforms import PaddingMode, to_channel_dimension_format\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/image_transforms.py\", line 48, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/__init__.py\", line 55, in <module>\n",
            "    from tensorflow._api.v2 import compat\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/__init__.py\", line 8, in <module>\n",
            "    from tensorflow._api.v2.compat import v1\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\", line 30, in <module>\n",
            "    from tensorflow._api.v2.compat.v1 import compat\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\", line 8, in <module>\n",
            "    from tensorflow._api.v2.compat.v1.compat import v1\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\", line 47, in <module>\n",
            "    from tensorflow._api.v2.compat.v1 import lite\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\", line 9, in <module>\n",
            "    from tensorflow._api.v2.compat.v1.lite import experimental\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\", line 8, in <module>\n",
            "    from tensorflow._api.v2.compat.v1.lite.experimental import authoring\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\", line 8, in <module>\n",
            "    from tensorflow.lite.python.authoring.authoring import compatible # line: 263\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/authoring/authoring.py\", line 42, in <module>\n",
            "    from tensorflow.lite.python import convert\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py\", line 29, in <module>\n",
            "    from tensorflow.lite.python import util\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/util.py\", line 53, in <module>\n",
            "    from jax import jit as _jit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jax/__init__.py\", line 39, in <module>\n",
            "    from jax import config as _config_module\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jax/config.py\", line 15, in <module>\n",
            "    from jax._src.config import config as _deprecated_config  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jax/_src/config.py\", line 28, in <module>\n",
            "    from jax._src import lib\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jax/_src/lib/__init__.py\", line 86, in <module>\n",
            "    import jaxlib.xla_client as xla_client\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/jaxlib/xla_client.py\", line 32, in <module>\n",
            "    from . import xla_extension as _xla\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not find Gemma3ForCausalLM neither in <module 'transformers.models.gemma3' from '/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/__init__.py'> nor in <module 'transformers' from '/usr/local/lib/python3.12/dist-packages/transformers/__init__.py'>!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not find {attr} in {transformers_module}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not find Gemma3ForCausalLM in <module 'transformers' from '/usr/local/lib/python3.12/dist-packages/transformers/__init__.py'>!",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-944602727.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgemma_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"google/functiongemma-270m-it\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m base_model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mgemma_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m             )\n\u001b[1;32m    600\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0msupported_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupported_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msupported_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;31m# Maybe there was several model types associated with this config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{module_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not find {attr} neither in {module} nor in {transformers_module}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not find {attr} in {transformers_module}!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not find Gemma3ForCausalLM neither in <module 'transformers.models.gemma3' from '/usr/local/lib/python3.12/dist-packages/transformers/models/gemma3/__init__.py'> nor in <module 'transformers' from '/usr/local/lib/python3.12/dist-packages/transformers/__init__.py'>!"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "gemma_model = \"google/functiongemma-270m-it\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    gemma_model,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\",\n",
        "    dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
        "\n",
        "print(f\"Device: {base_model.device}\")\n",
        "print(f\"DType:  {base_model.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hI4twbrz0xj"
      },
      "source": [
        "Device should print as `cuda` if you're using a GPU runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB0Fo8MRbxxY"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "To fine-tune FunctionGemma, we utilize the [Mobile Actions\n",
        "dataset](https://huggingface.co/datasets/google/mobile-actions), which is\n",
        "publicly available on Hugging Face. Each entry in this dataset provides:\n",
        "\n",
        "*   The set of tools (functions) the model can use:\n",
        "    1. Turn the flashlight on\n",
        "    2. Turn the flashlight off\n",
        "    3. Create a contact in the phone's contact list\n",
        "    4. Send an email\n",
        "    5. Show a location on the map\n",
        "    6. Open the WiFi settings\n",
        "    7. Create a new calendar event\n",
        "*   The system prompt providing the context like current date and time\n",
        "*   The user prompt, like `turn on the flashlight`.\n",
        "*   The expected model response, including the appropriate function calls."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create patch dataset to address specific error patterns\n",
        "\n",
        "Based on the evaluation results, we identified several error patterns:\n",
        "- Missing or incorrect tools (e.g., `open_wifi_settings`, flashlight operations)\n",
        "- Relative date parsing errors (\"this Thursday\", \"next Friday\")\n",
        "- Argument normalization issues (email addresses, map queries, titles)\n",
        "\n",
        "We'll create a small patch dataset (~100 examples) targeting these specific failures and combine it with the original dataset for improved accuracy."
      ],
      "metadata": {
        "id": "9TpSR8XgQJh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "\n",
        "# Create comprehensive patch examples targeting specific error patterns from eval\n",
        "# Covers: WiFi settings, flashlight ops, relative dates, email case, map queries, multi-tool\n",
        "\n",
        "# Extract failing examples from trained_scored DataFrame\n",
        "# failing_examples = trained_scored[trained_scored['correct'] == False].copy()\n",
        "\n",
        "# Target 75-140 examples focusing on specific error patterns\n",
        "# Using 'user' column for user prompts\n",
        "# error_patterns = {\n",
        "#    'wifi': failing_examples[failing_examples['user'].str.contains('wifi|Wi-Fi|Wi-fi|network', case=False, na=False)],\n",
        "#    'flashlight': failing_examples[failing_examples['user'].str.contains('flashlight|torch|light', case=False, na=False)],\n",
        "#    'relative_dates': failing_examples[failing_examples['user'].str.contains('thursday|friday|monday|tuesday|wednesday|saturday|sunday|tomorrow|today|next week', case=False, na=False)],\n",
        "#    'email': failing_examples[failing_examples['user'].str.contains('email|send|compose', case=False, na=False)],\n",
        "#    'map': failing_examples[failing_examples['user'].str.contains('map|location|address|navigate', case=False, na=False)],\n",
        "#    'multi_tool': failing_examples[failing_examples['target_text'].str.contains('><', case=False, na=False)]  # Multiple function calls\n",
        "# }\n",
        "\n",
        "# Build patch examples list - use the format expected by dataset\n",
        "patch_examples = []\n",
        "target_per_pattern = 20  # Aim for ~20 per pattern = 120 total\n",
        "\n",
        "# for pattern_name, pattern_df in error_patterns.items():\n",
        "    # Sample up to target_per_pattern from each error type\n",
        "#    sample_size = min(len(pattern_df), target_per_pattern)\n",
        "#    if sample_size > 0:\n",
        "#        sampled = pattern_df.sample(n=sample_size, random_state=42)\n",
        "#        for idx, row in sampled.iterrows():\n",
        "#            # Get the full example from eval_dataset using the same user prompt\n",
        "#            matching = [ex for ex in eval_dataset if ex['text'].find(row['user']) != -1]\n",
        "#            if matching:\n",
        "#                patch_examples.append({\"text\": matching[0]['text']})\n",
        "\n",
        "# print(f\"Created {len(patch_examples)} patch examples from failing cases\")\n",
        "# print(f\"Breakdown: WiFi={len(error_patterns['wifi'])}, Flashlight={len(error_patterns['flashlight'])}, Dates={len(error_patterns['relative_dates'])}, Email={len(error_patterns['email'])}, Map={len(error_patterns['map'])}, Multi-tool={len(error_patterns['multi_tool'])}\")\n",
        "\n",
        "# Create Dataset from patch examples\n",
        "# patch_ds = Dataset.from_dict({\"text\": [ex[\"text\"] for ex in patch_examples]})\n",
        "# print(f\"\\nPatch dataset size: {len(patch_ds)}\")\n",
        "# print(f\"Patch dataset sample:\")\n",
        "# print(json.dumps(json.loads(patch_ds[0]['text']), indent=2)[:500] + \"...\")"
      ],
      "metadata": {
        "id": "5jtBuxvFQH91"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine patch dataset with original dataset\n",
        "# Repeat patch examples 30 times to emphasize these patternspatch_ds_repeated = concatenate_datasets([patch_ds] * 30)\n",
        "# Combine with original dataset\n",
        "# enhanced_dataset = concatenate_datasets([dataset, patch_ds_repeated]).shuffle(seed=42)\n",
        "\n",
        "# print(f\"Original dataset size: {len(dataset)}\")\n",
        "# print(f\"Patch dataset size (repeated 30x): {len(patch_ds_repeated)}\")\n",
        "# print(f\"Enhanced dataset size: {len(enhanced_dataset)}\")\n",
        "# print(f\"\\nPatch data represents {len(patch_ds_repeated)/len(enhanced_dataset)*100:.2f}% of enhanced dataset\")\n",
        "\n",
        "# Update the dataset variable to use the enhanced version\n",
        "# dataset = enhanced_dataset"
      ],
      "metadata": {
        "id": "1KJ3ffsxQd5Q"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: The enhanced dataset now includes targeted patch examples to address specific error patterns observed in the initial evaluation. This approach:\n",
        "\n",
        "- Maintains the original dataset's diversity while emphasizing problematic patterns\n",
        "- Helps the model learn correct behavior for edge cases (flashlight, WiFi settings, relative dates)\n",
        "- Improves multi-tool calling accuracy\n",
        "- Should result in higher evaluation scores (>0.85) in subsequent fine-tuning runs\n",
        "\n",
        "You can expand the patch dataset with additional examples from your specific error analysis or skip this step if running the initial baseline fine-tune."
      ],
      "metadata": {
        "id": "lfi3RrO5QiGg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "N2AwsZMkRioU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e622537-7afd-42eb-c7bd-4e07b05959fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1mHere's an example from your dataset:\u001b[0m \n",
            "{\n",
            "  \"metadata\": \"train\",\n",
            "  \"tools\": [\n",
            "    {\n",
            "      \"function\": {\n",
            "        \"name\": \"turn_on_flashlight\",\n",
            "        \"description\": \"Turns the flashlight on.\",\n",
            "        \"parameters\": {\n",
            "          \"type\": \"OBJECT\",\n",
            "          \"properties\": {}\n",
            "        }\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"function\": {\n",
            "        \"name\": \"create_calendar_event\",\n",
            "        \"description\": \"Creates a new calendar event.\",\n",
            "        \"parameters\": {\n",
            "          \"type\": \"OBJECT\",\n",
            "          \"properties\": {\n",
            "            \"title\": {\n",
            "              \"type\": \"STRING\",\n",
            "              \"description\": \"The title of the event.\"\n",
            "            },\n",
            "            \"datetime\": {\n",
            "              \"type\": \"STRING\",\n",
            "              \"description\": \"The date and time of the event in the format YYYY-MM-DDTHH:MM:SS.\"\n",
            "            }\n",
            "          },\n",
            "          \"required\": [\n",
            "            \"title\",\n",
            "            \"datetime\"\n",
            "          ]\n",
            "        }\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"function\": {\n",
            "        \"name\": \"open_wifi_settings\",\n",
            "        \"description\": \"Opens the Wi-Fi settings.\",\n",
            "        \"parameters\": {\n",
            "          \"type\": \"OBJECT\",\n",
            "          \"properties\": {}\n",
            "        }\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"function\": {\n",
            "        \"name\": \"send_email\",\n",
            "        \"description\": \"Sends an email.\",\n",
            "        \"parameters\": {\n",
            "          \"type\": \"OBJECT\",\n",
            "          \"properties\": {\n",
            "            \"to\": {\n",
            "              \"type\": \"STRING\",\n",
            "              \"description\": \"The email address of the recipient.\"\n",
            "            },\n",
            "            \"subject\": {\n",
            "              \"type\": \"STRING\",\n",
            "              \"description\": \"The subject of the email.\"\n",
            "            },\n",
            "            \"body\": {\n",
            "              \"type\": \"STRING\",\n",
            "              \"description\": \"The body of the email.\"\n",
            "            }\n",
            "          },\n",
            "          \"required\": [\n",
            "            \"to\",\n",
            "            \"subject\"\n",
            "          ]\n",
            "        }\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"function\": {\n",
            "        \"name\": \"show_map\",\n",
            "        \"description\": \"Shows a location on the map.\",\n",
            "        \"parameters\": {\n",
            "          \"type\": \"OBJECT\",\n",
            "          \"properties\": {\n",
            "            \"query\": {\n",
            "              \"type\": \"STRING\",\n",
            "              \"description\": \"The location to search for. May be the name of a place, a business, or an address.\"\n",
            "            }\n",
            "          },\n",
            "          \"required\": [\n",
            "            \"query\"\n",
            "          ]\n",
            "        }\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"function\": {\n",
            "        \"name\": \"turn_off_flashlight\",\n",
            "        \"description\": \"Turns the flashlight off.\",\n",
            "        \"parameters\": {\n",
            "          \"type\": \"OBJECT\",\n",
            "          \"properties\": {}\n",
            "        }\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"function\": {\n",
            "        \"name\": \"create_contact\",\n",
            "        \"description\": \"Creates a contact in the phone's contact list.\",\n",
            "        \"parameters\": {\n",
            "          \"type\": \"OBJECT\",\n",
            "          \"properties\": {\n",
            "            \"email\": {\n",
            "              \"type\": \"STRING\",\n",
            "              \"description\": \"The email address of the contact.\"\n",
            "            },\n",
            "            \"last_name\": {\n",
            "              \"type\": \"STRING\",\n",
            "              \"description\": \"The last name of the contact.\"\n",
            "            },\n",
            "            \"first_name\": {\n",
            "              \"type\": \"STRING\",\n",
            "              \"description\": \"The first name of the contact.\"\n",
            "            },\n",
            "            \"phone_number\": {\n",
            "              \"type\": \"STRING\",\n",
            "              \"description\": \"The phone number of the contact.\"\n",
            "            }\n",
            "          },\n",
            "          \"required\": [\n",
            "            \"first_name\",\n",
            "            \"last_name\"\n",
            "          ]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"role\": \"developer\",\n",
            "      \"content\": \"Current date and time given in YYYY-MM-DDTHH:MM:SS format: 2024-06-23T01:50:59\\nDay of week is Sunday\\nYou are a model that can do function calling with the following functions\\n\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"Please save a new contact for Kenji Tanaka. His phone number is +81 3 1234 5678 and his email is kenji.tanaka@kmail.com.\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"assistant\",\n",
            "      \"tool_calls\": [\n",
            "        {\n",
            "          \"function\": {\n",
            "            \"name\": \"create_contact\",\n",
            "            \"arguments\": {\n",
            "              \"first_name\": \"Kenji\",\n",
            "              \"phone_number\": \"+81 3 1234 5678\",\n",
            "              \"last_name\": \"Tanaka\",\n",
            "              \"email\": \"kenji.tanaka@kmail.com\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from random import randint\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "data_file = hf_hub_download(repo_id=\"google/mobile-actions\", filename=\"dataset.jsonl\", repo_type=\"dataset\")\n",
        "dataset = load_dataset(\"text\", data_files=data_file, encoding=\"utf-8\")[\"train\"].shuffle()\n",
        "\n",
        "print(f\"\\n\\033[1mHere's an example from your dataset:\\033[0m \\n{json.dumps(json.loads(dataset[randint(0, len(dataset) - 1)]['text']), indent=2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PafivP8u1Gv9"
      },
      "source": [
        "## Process the dataset for training and evaluation\n",
        "\n",
        "Now that you've loaded your data, format the training dataset into [Prompt-completion](https://huggingface.co/docs/trl/main/en/dataset_formats#prompt-completion) format for more efficient training later (`completion_only_loss=True`). This means the model will only learn from the `completion` instead of the `prompt`.\n",
        "\n",
        "- `prompt` for the non-trainable parts\n",
        "- `completion` for the trainable parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "VWz32s5h074E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "677eb49a346f476ca415e70656d639cb",
            "7b55d461723d4c0f9e1ba6f0c48bd1b0",
            "1d6ad1f5f6bc4eddbe0eedec3853904c",
            "1b8ed666faf44348a8cedb7e2378ec58",
            "f4a978205df3449493eb25e1c7dc1e98",
            "898e657f4eaf4d0d94b791ee9f72c8ff",
            "9f09181738504e9994f6b4cc9e635a00",
            "001b438a8d0c487fa52e1d02cc39bc73",
            "1838b294057946cfb4737abb8aef8351",
            "57ba86da0fd94424b29f7b68ab57b1f0",
            "580f039a15644295a8dd5c410ebb4d59"
          ]
        },
        "outputId": "87d23e7e-8441-47ea-fa6c-18c11824eaa8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9654 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "677eb49a346f476ca415e70656d639cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3058266809.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m   }\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mprocessed_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m         }\n\u001b[1;32m    561\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3339\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0munprocessed_kwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_kwargs_per_job\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3341\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0munprocessed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3342\u001b[0m                             \u001b[0mcheck_if_shard_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3671\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3672\u001b[0m                     \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3673\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3674\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3675\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36miter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3644\u001b[0m                     \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3645\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3646\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3647\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2491\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2493\u001b[0;31m                 yield self._getitem(\n\u001b[0m\u001b[1;32m   2494\u001b[0m                     \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2855\u001b[0m         \u001b[0mformat_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2856\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2857\u001b[0;31m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2858\u001b[0m         formatted_output = format_table(\n\u001b[1;32m   2859\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_query_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_query_table_with_indices_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpa_subtable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m_query_table_with_indices_mapping\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_query_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m_query_table\u001b[0;34m(table, key)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/table.py\u001b[0m in \u001b[0;36mfast_slice\u001b[0;34m(self, offset, length)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_interpolation_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def apply_format(sample):\n",
        "  template_iputs = json.loads(sample['text'])\n",
        "\n",
        "  prompt_and_completion = tokenizer.apply_chat_template(\n",
        "    template_iputs['messages'],\n",
        "    tools=template_iputs['tools'],\n",
        "    tokenize=False,\n",
        "    # add_generation_prompt is False since we don't need model output after all\n",
        "    # messages.\n",
        "    add_generation_prompt=False)\n",
        "\n",
        "  prompt = tokenizer.apply_chat_template(\n",
        "    template_iputs['messages'][:-1],\n",
        "    tools=template_iputs['tools'],\n",
        "    tokenize=False,\n",
        "    # add_generation_prompt is True since we would like to include\n",
        "    # \"<start_of_turn>model\" in the prompt, if needed.\n",
        "    add_generation_prompt=True)\n",
        "\n",
        "  completion = prompt_and_completion[len(prompt):]\n",
        "\n",
        "  return {\n",
        "     \"prompt\": prompt,\n",
        "     \"completion\": completion,\n",
        "     \"split\": template_iputs[\"metadata\"],\n",
        "  }\n",
        "\n",
        "processed_dataset = dataset.map(apply_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWf9Vlfm9tkh"
      },
      "outputs": [],
      "source": [
        "#@title Review the processed dataset\n",
        "\n",
        "print(\"\\033[1mHere's an example from the formatted dataset:\\033[0m\")\n",
        "print(json.dumps(processed_dataset[randint(0, len(processed_dataset) - 1)], indent=2))\n",
        "\n",
        "longest_example = max(processed_dataset, key=lambda example: len(example['prompt'] + example['completion']))\n",
        "longest_example_token_count = len(tokenizer.tokenize(longest_example['prompt'] + longest_example['completion']))\n",
        "\n",
        "print(f\"\\n\\033[1mThe longest example length is {len(longest_example['prompt'] + longest_example['completion'])} with {longest_example_token_count} tokens. We need to set the max_length larger than the token count in SFTConfig below.\\033[0m\")\n",
        "print(json.dumps(longest_example, indent=2))\n",
        "\n",
        "max_token_count = longest_example_token_count + 100\n",
        "print(f\"\\n\\033[1mUsing max_token_count of {max_token_count} (= {longest_example_token_count} + 100) for training below.\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QPMjN_vwQf5"
      },
      "outputs": [],
      "source": [
        "#@title Prepare train and eval dataset.\n",
        "\n",
        "train_dataset = processed_dataset.filter(lambda example: example['split'] == 'train')\n",
        "eval_dataset = processed_dataset.filter(lambda example: example['split'] == 'eval')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3w3b9-O4fDz"
      },
      "source": [
        "## Recommended: Test the base model\n",
        "\n",
        "Now, we have loaded both the base model and the dataset. Let's first check how the base model's ability to respond to  \n",
        "a random sample.\n",
        "\n",
        "Try testing it a few times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S8X9LBKYD3f"
      },
      "outputs": [],
      "source": [
        "#@title Test with a prompt\n",
        "\n",
        "from transformers import pipeline\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "# Create a transformers inference pipeline\n",
        "pipe = pipeline(\"text-generation\", model=gemma_model, tokenizer=tokenizer)\n",
        "\n",
        "user_prompt = \"Schedule a \\\"team meeting\\\" tomorrow at 4pm.\"  #@param {type:\"string\"}\n",
        "messages = [\n",
        "    {\"role\": \"developer\", \"content\": \"Current date and time given in YYYY-MM-DDTHH:MM:SS format: 2024-11-15T05:59:00. You are a model that can do function calling with the following functions\"},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "]\n",
        "\n",
        "# Reuse the tools from the sample\n",
        "tools = json.loads(dataset[0]['text'])['tools']\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tools=tools,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True)\n",
        "\n",
        "print(f\"\\n\\033[1mPrompt:\\033[0m {user_prompt}\")\n",
        "output = pipe(prompt, max_new_tokens=max_token_count)\n",
        "model_output = output[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "print(f\"\\n\\033[1mBase model output:\\033[0m {model_output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNfNON0yBJlg"
      },
      "source": [
        "Note that how the base model is unable to successfully call the `create_calendar_event` function for this prompt.\n",
        "\n",
        "Now, we will pick a sample from the training dataset and see how it performs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvgTeVQCAcxe"
      },
      "source": [
        "## Test with training dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8L0_INJyUok"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "# Create a transformers inference pipeline\n",
        "pipe = pipeline(\"text-generation\", model=gemma_model, tokenizer=tokenizer)\n",
        "\n",
        "# Select a random sample from the test dataset\n",
        "rand_idx = randint(0, len(train_dataset) - 1)\n",
        "test_sample = train_dataset[rand_idx]\n",
        "\n",
        "input_prompt = test_sample['prompt']\n",
        "expected_output = test_sample['completion']\n",
        "\n",
        "# Generate the output\n",
        "output = pipe(input_prompt, max_new_tokens=max_token_count, skip_special_tokens=False)\n",
        "actual_output = output[0]['generated_text'][len(input_prompt):].strip()\n",
        "\n",
        "print(f\"\\n\\033[1mInput prompt\\033[0m   : {input_prompt}\")\n",
        "print(f\"\\n\\033[1mExpected output\\033[0m: {expected_output}\")\n",
        "print(f\"\\n\\033[1mActual output\\033[0m  : {actual_output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph26HDJgua3W"
      },
      "source": [
        "The base model output may not meet your expectationsand that's okay!\n",
        "\n",
        "FunctionGemma 270M was designed for task specialization, which means it can improve performance for specific tasks when trained with representative examples. Let's fine-tune the model for more reliable outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbd9fc1b"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "Hugging Face [TRL](https://huggingface.co/docs/trl/index) provides tools for training and fine-tuning LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BJFoOdL0y8w"
      },
      "source": [
        "### Configure the tuning job\n",
        "Define the training configuration for the FunctionGemma base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiIj1ADc-exw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from trl import SFTConfig\n",
        "\n",
        "output_dir = \"/content/mobile-actions-functiongemma\"  # Where to save your fine-tuned checkpoints\n",
        "tokenizer = AutoTokenizer.from_pretrained(gemma_model)\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=output_dir,                            # Directory to save adapters\n",
        "    num_train_epochs=4,                               # Number of training epochs\n",
        "    per_device_train_batch_size=8,                    # Batch size per device during training\n",
        "    gradient_accumulation_steps=4,                    # Gradient accumulation during training\n",
        "    logging_strategy=\"steps\",                         # Log every steps\n",
        "    eval_strategy=\"steps\",                            # Evaluate loss metrics based on steps\n",
        "    eval_steps=50,                                    # Evaluate loss metrics every 50 steps\n",
        "    logging_steps=50,                                 # Log loss metrics every 50 steps\n",
        "    save_strategy=\"steps\",                            # Save checkpoint every epoch\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",                       # Cosine scheduler is often better for full FT\n",
        "    max_length=max_token_count,                       # Max sequence length for model and packing of the dataset\n",
        "    gradient_checkpointing=True,                      # Use gradient checkpointing to save memory\n",
        "    packing=False,                                    # Groups multiple samples in the dataset into a single sequence\n",
        "    optim=\"adamw_torch_fused\",                        # Use fused adamw optimizer\n",
        "    bf16=True,                                        # Use bf16 for mixed precision training\n",
        "    completion_only_loss=True,                        # Train on completion only to improve quality\n",
        "    report_to=\"none\",                                 # No reporting.\n",
        "    load_best_model_at_end=True,                      # Load the best model at the end of training\n",
        "    metric_for_best_model=\"eval_loss\",                     # Use loss to evaluate best model\n",
        "    greater_is_better=False,                          # Lower loss is better\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    gemma_model,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.bfloat16,\n",
        "    attn_implementation='eager')\n",
        "\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(\"Training configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd88e798"
      },
      "source": [
        "### Start training\n",
        "\n",
        "`SFTTrainer` tokenizes the datasets and trains the base model using the hyperparameters from the previous step.\n",
        "\n",
        "The training time varies based on a range of factors, such as the size of your dataset or number of epochs. Using a A100 GPU, this takes about 8 minutes for 1 epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqacJNeU9v7b"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "# Train and save the fine-tuned model\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Fine-tuned model saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDvGlb5xO34z"
      },
      "source": [
        "The weights for each training checkpoint (epoch) will be saved in your temporary Colab session storage. Now, you can evaluate the training and validation loss metrics to choose which checkpoint to for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xll8zZ3_u8Mt"
      },
      "source": [
        "### Plot training results\n",
        "To evaluate the model, you can plot the training and validation losses using Matplotlib to visualize these metrics over training steps or epochs. This helps monitor the training process and make informed decisions about which hyperparameters to adjust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPN-DTopaUIy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Access the log history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Extract training / validation loss\n",
        "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
        "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
        "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
        "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
        "\n",
        "# Plot the training loss\n",
        "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
        "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss per Epoch\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf86e31d"
      },
      "source": [
        "### Test the fine-tuned model\n",
        "\n",
        "Let's compare your fine-tuned model performance against the base model! Test a few inputs by updating `user_prompt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28R3pRN_hai7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Create Transformers inference pipeline\n",
        "trained_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "pipe = pipeline(\"text-generation\", model=trained_model, tokenizer=tokenizer)\n",
        "pipe_base = pipeline(\"text-generation\", model=gemma_model, device_map=\"auto\")\n",
        "\n",
        "# Test a prompt\n",
        "user_prompt = \"schedule a meeting for 4pm tomorrow\"  #@param {type:\"string\"}\n",
        "messages = [\n",
        "    {\"role\": \"developer\", \"content\": \"Current date and time given in YYYY-MM-DDTHH:MM:SS format: 2024-11-15T05:59:00. You are a model that can do function calling with the following functions\"},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "]\n",
        "\n",
        "# Reuse the tools from the sample\n",
        "tools = json.loads(dataset[0]['text'])['tools']\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tools=tools,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True)\n",
        "\n",
        "print(f\"\\n\\033[1mPrompt:\\033[0m {prompt}\")\n",
        "output = pipe(prompt, max_new_tokens=max_token_count)\n",
        "output_base = pipe_base(prompt, max_new_tokens=max_token_count)\n",
        "model_output = output[0]['generated_text'][len(prompt):].strip()\n",
        "model_output_base = output_base[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "print(f\"\\n\\033[1mFine-tuned model output:\\033[0m {model_output}\")\n",
        "\n",
        "print(f\"\\n\\033[1mBase model output:\\033[0m       {model_output_base}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmpAI15UeGcs"
      },
      "source": [
        "### Evaluate the fine-tuned model\n",
        "\n",
        "Evaluating a fine-tuned model is essential to ensure that the process actually improved the model's performance without introducing new issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6HmcMawflSP"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions for evaluation\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def extract_function_call(model_output):\n",
        "    \"\"\"\n",
        "    Parses a string containing specific function call markers and returns\n",
        "    a list of function call objects. Here is an example of the obejct:\n",
        "\n",
        "    ```\n",
        "    <start_function_call>call:open_map{query:<escape>San Francisco<escape>}<end_function_call>\n",
        "    ```\n",
        "\n",
        "    Args:\n",
        "        model_output (str): The model output string.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries representing the function calls.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Pattern to extract the full content of a single function call\n",
        "    # Flags: DOTALL allows matching across newlines if necessary\n",
        "    call_pattern = r\"<start_function_call>(.*?)<end_function_call>\"\n",
        "    raw_calls = re.findall(call_pattern, model_output, re.DOTALL)\n",
        "\n",
        "    for raw_call in raw_calls:\n",
        "        # Check if the content starts with 'call:'\n",
        "        if not raw_call.strip().startswith(\"call:\"):\n",
        "            continue\n",
        "\n",
        "        # Extract function name\n",
        "        # Expected format: call:func_name{...}\n",
        "        try:\n",
        "            # Split only on the first brace to separate name and args\n",
        "            pre_brace, args_segment = raw_call.split(\"{\", 1)\n",
        "\n",
        "            function_name = pre_brace.replace(\"call:\", \"\").strip()\n",
        "\n",
        "            # Remove the trailing closing brace '}'\n",
        "            args_content = args_segment.strip()\n",
        "            if args_content.endswith(\"}\"):\n",
        "                args_content = args_content[:-1]\n",
        "\n",
        "            arguments = {}\n",
        "\n",
        "            # Pattern to extract arguments\n",
        "            # Looks for: key:<escape>value<escape>\n",
        "            # The key pattern [^:,]* ensures we don't accidentally eat previous commas\n",
        "            arg_pattern = r\"(?P<key>[^:,]*?):<escape>(?P<value>.*?)<escape>\"\n",
        "\n",
        "            arg_matches = re.finditer(arg_pattern, args_content, re.DOTALL)\n",
        "\n",
        "            for match in arg_matches:\n",
        "                key = match.group(\"key\").strip()\n",
        "                value = match.group(\"value\")\n",
        "                arguments[key] = value\n",
        "\n",
        "            results.append({\n",
        "                \"function\": {\n",
        "                    \"name\": function_name,\n",
        "                    \"arguments\": arguments\n",
        "                }\n",
        "            })\n",
        "\n",
        "        except ValueError:\n",
        "            # Handles cases where syntax might be malformed (e.g., missing '{')\n",
        "            continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def extract_text(model_output):\n",
        "    \"\"\"\n",
        "    Extracts text content and removing the <end_of_turn> marker.\n",
        "\n",
        "    Args:\n",
        "        model_output (str): The model output string.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    if not model_output or model_output.startswith(\"<start_function_call>\"):\n",
        "        return None\n",
        "    return model_output.replace(\"<end_of_turn>\", \"\").strip()\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "def get_eval_logs(dataset, pipe):\n",
        "  batch_size = 1\n",
        "  logs = []\n",
        "  # Select a random sample from the test dataset\n",
        "  for i, output in enumerate(pipe(KeyDataset(dataset, \"prompt\"), batch_size=batch_size)):\n",
        "    orig_data = dataset[i]['text']\n",
        "    messages = json.loads(orig_data)['messages']\n",
        "    user_message = messages[1]\n",
        "    assistant_first_message = messages[2]\n",
        "    input_prompt = dataset[i]['prompt']\n",
        "    # Generate the output\n",
        "    model_output_only = output[0]['generated_text'][len(input_prompt):].strip()\n",
        "\n",
        "    logs.append(\n",
        "        {\n",
        "            # The original user prompt/query.\n",
        "            \"user\": user_message['content'],\n",
        "\n",
        "            # List of ground truth function call objects.\n",
        "            \"target_fc\": assistant_first_message.get('tool_calls', []),\n",
        "\n",
        "            # Ground truth text response.\n",
        "            \"target_text\": assistant_first_message.get('content'),\n",
        "\n",
        "            # List of model-generated function call objects.\n",
        "            \"output_fc\": extract_function_call(model_output_only),\n",
        "\n",
        "            # Model-generated text response.\n",
        "            \"output_text\": extract_text(model_output_only),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if (i + 1) % batch_size == 0:\n",
        "      print(f\"Eval process: {(i + 1) * 100.0 / len(dataset):.2f}%\")\n",
        "  return logs\n",
        "\n",
        "def get_scored_data_frame(dataset, pipe):\n",
        "  logs = get_eval_logs(dataset, pipe)\n",
        "  logs_df = pd.DataFrame.from_records(logs)\n",
        "\n",
        "  scored = pd.DataFrame()\n",
        "  scored['user'] = logs_df['user']\n",
        "  scored['target_names'] = logs_df['target_fc'].apply(lambda x: [fc['function']['name'] for fc in x])\n",
        "  scored['output_names'] = logs_df['output_fc'].apply(lambda x: [fc['function']['name'] for fc in x])\n",
        "  scored[\"target_arguments\"] = logs_df['target_fc'].apply(lambda x: [dict(sorted(fc['function']['arguments'].items())) for fc in x])\n",
        "  scored[\"output_arguments\"] = logs_df['output_fc'].apply(lambda x: [dict(sorted(fc['function']['arguments'].items())) for fc in x])\n",
        "  scored['target_text'] = logs_df['target_text']\n",
        "  scored['output_text'] = logs_df['output_text']\n",
        "  scored[\"correct_names\"] = scored[\"target_names\"] == scored[\"output_names\"]\n",
        "  scored[\"correct_arguments\"] = scored[\"target_arguments\"] == scored[\"output_arguments\"]\n",
        "  scored[\"correct\"] = scored[\"correct_names\"] & scored[\"correct_arguments\"]\n",
        "\n",
        "  return scored\n",
        "\n",
        "def review(scored):\n",
        "  scored[\"incorrect_names\"] = scored[\"target_names\"] != scored[\"output_names\"]\n",
        "  scored[\"incorrect_arguments\"] = scored[\"target_arguments\"] != scored[\"output_arguments\"]\n",
        "  scored[\"incorrect\"] = scored[\"incorrect_names\"] | scored[\"incorrect_arguments\"]\n",
        "\n",
        "  for index, row in scored[scored[\"incorrect\"]].iterrows():\n",
        "    print(f\"\\033[1mSample #{index} prompt  \\033[0m: {row[\"user\"]}\")\n",
        "    print(f\"\\033[1mSample #{index} expected\\033[0m: {row[\"target_names\"]}, {row[\"target_arguments\"]}\")\n",
        "    print(f\"\\033[1mSample #{index} actual  \\033[0m: {row[\"output_names\"]}, {row[\"output_arguments\"]}\")\n",
        "    print(\"---------------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eGq4ZsjjoTl"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate the base model\n",
        "\n",
        "base_scored = get_scored_data_frame(\n",
        "    eval_dataset,\n",
        "    pipeline(\"text-generation\", model=gemma_model, device_map=\"auto\", temperature = 0.001),\n",
        ")\n",
        "\n",
        "base_scored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocOK88aEG6R6"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate the fine-tuned model\n",
        "\n",
        "from transformers import pipeline\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "# Create a transformers inference pipeline\n",
        "trained_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "trained_scored = get_scored_data_frame(\n",
        "    eval_dataset,\n",
        "    pipeline(\"text-generation\", model=trained_model, tokenizer=tokenizer, temperature = 0.001)\n",
        ")\n",
        "\n",
        "trained_scored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMY1P8Rtyopt"
      },
      "outputs": [],
      "source": [
        "#@title Compare the score of the base and fine-tuned models\n",
        "\n",
        "# Optional: save the score in json file\n",
        "trained_scored.to_json('scored_df_20251215_trained.json')\n",
        "base_scored.to_json('scored_df_20251215_base.json')\n",
        "\n",
        "print(f\"\\033[1mBase model score\\033[0m       : {base_scored[\"correct\"].mean()}\")\n",
        "print(f\"\\033[1mFine-tuned model score\\033[0m : {trained_scored[\"correct\"].mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f-3QvCShs5a"
      },
      "source": [
        "## Review what are not matching\n",
        "\n",
        "The score about tell how much eval data get exactly correct function calls. It gives the lower bound of correctness. Some output might not match the eval date but still acceptable. For example, the `show_map` function call below:\n",
        "\n",
        "* show_map:{'query': 'Maison Marulaz, Besanon, France'}\n",
        "* show_map:{'query': 'Maison Marulaz in Besanon, France'}\n",
        "\n",
        "Let's take a look at the eval date without exact match. Many of them could also be acceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3-Iwa09zC_K"
      },
      "outputs": [],
      "source": [
        "review(trained_scored)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86qPcFbHH_kh"
      },
      "source": [
        "Does the model call the function you'd expect?\n",
        "\n",
        "If you're not getting the results you want, you can try [using different hyperparameters](#scrollTo=-BJFoOdL0y8w) to train the model, or updating your training dataset to contain more representative examples.\n",
        "\n",
        "Once you're happy with the results, you can save your model to Hugging Face Hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H12D9g4X_peV"
      },
      "source": [
        "## Save your model and upload to Hugging Face Hub\n",
        "**You now have a customized FunctionGemma 270M model! **\n",
        "\n",
        "Upload it to a repository on Hugging Face Hub so you easily share your model or access it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbeyDcpwi4IB"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import ModelCard, ModelCardData, whoami\n",
        "\n",
        "trained_model = AutoModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "#@markdown Name your model\n",
        "model_name = \"mobile-actions_jprtr\"    #@param {type:\"string\"}\n",
        "\n",
        "username = whoami()['name']\n",
        "hf_repo_id = f\"{username}/functiongemma-270m-it-{model_name}\"\n",
        "\n",
        "repo_url = trained_model.push_to_hub(hf_repo_id, create_repo=True, commit_message=\"Upload model\")\n",
        "tokenizer.push_to_hub(hf_repo_id)\n",
        "\n",
        "card_content = f\"\"\"\n",
        "---\n",
        "base_model: {gemma_model}\n",
        "tags:\n",
        "- function-calling\n",
        "- mobile-actions\n",
        "- gemma\n",
        "---\n",
        "A fine-tuned model based on `{gemma_model}`.\"\"\"\n",
        "card = ModelCard(card_content)\n",
        "\n",
        "card.push_to_hub(hf_repo_id)\n",
        "\n",
        "print(f\"Uploaded to {repo_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePMIkvtMT24J"
      },
      "source": [
        "## Conversion to .litertlm for on-device deployment\n",
        "\n",
        "The first step is to install the necessary libraries using the `pip` package installer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "No47TBwEjroB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d329aeaf-4b7b-4407-e2f3-2eea5e009614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting ai-edge-torch-nightly\n",
            "  Using cached ai_edge_torch_nightly-0.8.0.dev20251224-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting absl-py (from ai-edge-torch-nightly)\n",
            "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting numpy (from ai-edge-torch-nightly)\n",
            "  Using cached numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting scipy (from ai-edge-torch-nightly)\n",
            "  Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting safetensors (from ai-edge-torch-nightly)\n",
            "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting multipledispatch (from ai-edge-torch-nightly)\n",
            "  Using cached multipledispatch-1.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting transformers (from ai-edge-torch-nightly)\n",
            "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting kagglehub (from ai-edge-torch-nightly)\n",
            "  Using cached kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting tabulate (from ai-edge-torch-nightly)\n",
            "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting torch>=2.4.0 (from ai-edge-torch-nightly)\n",
            "  Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting tf-nightly>=2.21.0.dev20250818 (from ai-edge-torch-nightly)\n",
            "  Using cached tf_nightly-2.21.0.dev20251224-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting ai-edge-litert-nightly (from ai-edge-torch-nightly)\n",
            "  Using cached ai_edge_litert_nightly-2.2.0.dev20251223-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting ai-edge-quantizer-nightly (from ai-edge-torch-nightly)\n",
            "  Using cached ai_edge_quantizer_nightly-0.5.0.dev20251225-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting jax (from ai-edge-torch-nightly)\n",
            "  Using cached jax-0.8.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting torch-xla2>=0.0.1.dev20241201 (from torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch-nightly)\n",
            "  Using cached torch_xla2-0.0.1.dev202412041639-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting astunparse>=1.6.0 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=25.9.23 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting google_pasta>=0.1.1 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting opt_einsum>=2.3.2 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting packaging (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting protobuf<8.0.0,>=6.31.1 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting requests<3,>=2.21.0 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting setuptools (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting six>=1.12.0 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting termcolor>=1.1.0 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typing_extensions>=3.6.6 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting wrapt>=1.11.0 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting tb-nightly~=2.20.0.a (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached tb_nightly-2.20.0a20250717-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting keras-nightly>=3.10.0.dev (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached keras_nightly-3.14.0.dev2025122404-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting h5py<3.15.0,>=3.11.0 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting filelock (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached filelock-3.20.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting sympy>=1.13.3 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx>=2.5.1 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jinja2 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec>=0.8.5 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting immutabledict (from torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch-nightly)\n",
            "  Using cached immutabledict-4.2.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pytest (from torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch-nightly)\n",
            "  Using cached pytest-9.0.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting jaxlib<=0.8.2,>=0.8.2 (from jax->ai-edge-torch-nightly)\n",
            "  Using cached jaxlib-0.8.2-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting backports.strenum (from ai-edge-litert-nightly->ai-edge-torch-nightly)\n",
            "  Using cached backports_strenum-1.2.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting tqdm (from ai-edge-litert-nightly->ai-edge-torch-nightly)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting pyyaml (from kagglehub->ai-edge-torch-nightly)\n",
            "  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers->ai-edge-torch-nightly)\n",
            "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers->ai-edge-torch-nightly)\n",
            "  Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers->ai-edge-torch-nightly)\n",
            "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers->ai-edge-torch-nightly)\n",
            "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting rich (from keras-nightly>=3.10.0.dev->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting namex (from keras-nightly>=3.10.0.dev->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
            "Collecting optree (from keras-nightly>=3.10.0.dev->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting markdown>=2.6.8 (from tb-nightly~=2.20.0.a->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pillow (from tb-nightly~=2.20.0.a->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tb-nightly~=2.20.0.a->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tb-nightly~=2.20.0.a->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.4.0->ai-edge-torch-nightly)\n",
            "  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting iniconfig>=1.0.1 (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch-nightly)\n",
            "  Using cached iniconfig-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pluggy<2,>=1.5 (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch-nightly)\n",
            "  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting pygments>=2.7.2 (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai-edge-torch-nightly)\n",
            "  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->keras-nightly>=3.10.0.dev->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras-nightly>=3.10.0.dev->tf-nightly>=2.21.0.dev20250818->ai-edge-torch-nightly)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Using cached ai_edge_torch_nightly-0.8.0.dev20251224-py3-none-any.whl (459 kB)\n",
            "Using cached tf_nightly-2.21.0.dev20251224-cp312-cp312-manylinux_2_27_x86_64.whl (568.6 MB)\n",
            "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Using cached numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Using cached triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "Using cached torch_xla2-0.0.1.dev202412041639-py3-none-any.whl (76 kB)\n",
            "Using cached jax-0.8.2-py3-none-any.whl (2.9 MB)\n",
            "Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
            "Using cached ai_edge_litert_nightly-2.2.0.dev20251223-cp312-cp312-manylinux_2_27_x86_64.whl (9.6 MB)\n",
            "Using cached ai_edge_quantizer_nightly-0.5.0.dev20251225-py3-none-any.whl (212 kB)\n",
            "Using cached kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
            "Using cached multipledispatch-1.0.0-py3-none-any.whl (12 kB)\n",
            "Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Using cached flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
            "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
            "Using cached gast-0.7.0-py3-none-any.whl (22 kB)\n",
            "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Using cached grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
            "Using cached h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "Using cached jaxlib-0.8.2-cp312-cp312-manylinux_2_27_x86_64.whl (80.1 MB)\n",
            "Using cached keras_nightly-3.14.0.dev2025122404-py3-none-any.whl (1.5 MB)\n",
            "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "Using cached ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
            "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Using cached protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
            "Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached tb_nightly-2.20.0a20250717-py3-none-any.whl (5.5 MB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
            "Using cached backports_strenum-1.2.8-py3-none-any.whl (7.9 kB)\n",
            "Using cached filelock-3.20.1-py3-none-any.whl (16 kB)\n",
            "Using cached immutabledict-4.2.2-py3-none-any.whl (4.7 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached pytest-9.0.2-py3-none-any.whl (374 kB)\n",
            "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
            "Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Using cached iniconfig-2.3.0-py3-none-any.whl (7.5 kB)\n",
            "Using cached markdown-3.10-py3-none-any.whl (107 kB)\n",
            "Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
            "Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
            "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
            "Using cached werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
            "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
            "Using cached optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
            "Using cached pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, namex, multipledispatch, mpmath, libclang, flatbuffers, wrapt, wheel, urllib3, typing_extensions, triton, tqdm, termcolor, tensorboard-data-server, tabulate, sympy, six, setuptools, safetensors, regex, pyyaml, pygments, protobuf, pluggy, pillow, packaging, opt_einsum, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mdurl, MarkupSafe, markdown, iniconfig, immutabledict, idna, hf-xet, gast, fsspec, filelock, charset_normalizer, certifi, backports.strenum, absl-py, werkzeug, scipy, requests, pytest, optree, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, ml_dtypes, markdown-it-py, jinja2, h5py, grpcio, google_pasta, astunparse, ai-edge-litert-nightly, tb-nightly, rich, nvidia-cusolver-cu12, kagglehub, jaxlib, huggingface-hub, ai-edge-quantizer-nightly, torch, tokenizers, keras-nightly, jax, transformers, torch-xla2, tf-nightly, ai-edge-torch-nightly\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
            "  Attempting uninstall: namex\n",
            "    Found existing installation: namex 0.1.0\n",
            "    Uninstalling namex-0.1.0:\n",
            "      Successfully uninstalled namex-0.1.0\n",
            "  Attempting uninstall: multipledispatch\n",
            "    Found existing installation: multipledispatch 1.0.0\n",
            "    Uninstalling multipledispatch-1.0.0:\n",
            "      Successfully uninstalled multipledispatch-1.0.0\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: libclang\n",
            "    Found existing installation: libclang 18.1.1\n",
            "    Uninstalling libclang-18.1.1:\n",
            "      Successfully uninstalled libclang-18.1.1\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 25.12.19\n",
            "    Uninstalling flatbuffers-25.12.19:\n",
            "      Successfully uninstalled flatbuffers-25.12.19\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 2.0.1\n",
            "    Uninstalling wrapt-2.0.1:\n",
            "      Successfully uninstalled wrapt-2.0.1\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.45.1\n",
            "    Uninstalling wheel-0.45.1:\n",
            "      Successfully uninstalled wheel-0.45.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.6.2\n",
            "    Uninstalling urllib3-2.6.2:\n",
            "      Successfully uninstalled urllib3-2.6.2\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.1\n",
            "    Uninstalling triton-3.5.1:\n",
            "      Successfully uninstalled triton-3.5.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 3.2.0\n",
            "    Uninstalling termcolor-3.2.0:\n",
            "      Successfully uninstalled termcolor-3.2.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: tabulate\n",
            "    Found existing installation: tabulate 0.9.0\n",
            "    Uninstalling tabulate-0.9.0:\n",
            "      Successfully uninstalled tabulate-0.9.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 80.9.0\n",
            "    Uninstalling setuptools-80.9.0:\n",
            "      Successfully uninstalled setuptools-80.9.0\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.7.0\n",
            "    Uninstalling safetensors-0.7.0:\n",
            "      Successfully uninstalled safetensors-0.7.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2025.11.3\n",
            "    Uninstalling regex-2025.11.3:\n",
            "      Successfully uninstalled regex-2025.11.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.3\n",
            "    Uninstalling PyYAML-6.0.3:\n",
            "      Successfully uninstalled PyYAML-6.0.3\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.19.2\n",
            "    Uninstalling Pygments-2.19.2:\n",
            "      Successfully uninstalled Pygments-2.19.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 6.33.2\n",
            "    Uninstalling protobuf-6.33.2:\n",
            "      Successfully uninstalled protobuf-6.33.2\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 1.6.0\n",
            "    Uninstalling pluggy-1.6.0:\n",
            "      Successfully uninstalled pluggy-1.6.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 12.0.0\n",
            "    Uninstalling pillow-12.0.0:\n",
            "      Successfully uninstalled pillow-12.0.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: opt_einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
            "    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-nvshmem-cu12\n",
            "    Found existing installation: nvidia-nvshmem-cu12 3.3.20\n",
            "    Uninstalling nvidia-nvshmem-cu12-3.3.20:\n",
            "      Successfully uninstalled nvidia-nvshmem-cu12-3.3.20\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
            "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.13.1.3\n",
            "    Uninstalling nvidia-cufile-cu12-1.13.1.3:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.13.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.4.0\n",
            "    Uninstalling numpy-2.4.0:\n",
            "      Successfully uninstalled numpy-2.4.0\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.6.1\n",
            "    Uninstalling networkx-3.6.1:\n",
            "      Successfully uninstalled networkx-3.6.1\n",
            "  Attempting uninstall: mdurl\n",
            "    Found existing installation: mdurl 0.1.2\n",
            "    Uninstalling mdurl-0.1.2:\n",
            "      Successfully uninstalled mdurl-0.1.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.10\n",
            "    Uninstalling Markdown-3.10:\n",
            "      Successfully uninstalled Markdown-3.10\n",
            "  Attempting uninstall: iniconfig\n",
            "    Found existing installation: iniconfig 2.3.0\n",
            "    Uninstalling iniconfig-2.3.0:\n",
            "      Successfully uninstalled iniconfig-2.3.0\n",
            "  Attempting uninstall: immutabledict\n",
            "    Found existing installation: immutabledict 4.2.2\n",
            "    Uninstalling immutabledict-4.2.2:\n",
            "      Successfully uninstalled immutabledict-4.2.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "  Attempting uninstall: hf-xet\n",
            "    Found existing installation: hf-xet 1.2.0\n",
            "    Uninstalling hf-xet-1.2.0:\n",
            "      Successfully uninstalled hf-xet-1.2.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.7.0\n",
            "    Uninstalling gast-0.7.0:\n",
            "      Successfully uninstalled gast-0.7.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.10.0\n",
            "    Uninstalling fsspec-2025.10.0:\n",
            "      Successfully uninstalled fsspec-2025.10.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.20.1\n",
            "    Uninstalling filelock-3.20.1:\n",
            "      Successfully uninstalled filelock-3.20.1\n",
            "  Attempting uninstall: charset_normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.4\n",
            "    Uninstalling charset-normalizer-3.4.4:\n",
            "      Successfully uninstalled charset-normalizer-3.4.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.11.12\n",
            "    Uninstalling certifi-2025.11.12:\n",
            "      Successfully uninstalled certifi-2025.11.12\n",
            "  Attempting uninstall: backports.strenum\n",
            "    Found existing installation: backports.strenum 1.2.8\n",
            "    Uninstalling backports.strenum-1.2.8:\n",
            "      Successfully uninstalled backports.strenum-1.2.8\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 2.3.1\n",
            "    Uninstalling absl-py-2.3.1:\n",
            "      Successfully uninstalled absl-py-2.3.1\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.4\n",
            "    Uninstalling Werkzeug-3.1.4:\n",
            "      Successfully uninstalled Werkzeug-3.1.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.5\n",
            "    Uninstalling requests-2.32.5:\n",
            "      Successfully uninstalled requests-2.32.5\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 9.0.2\n",
            "    Uninstalling pytest-9.0.2:\n",
            "      Successfully uninstalled pytest-9.0.2\n",
            "  Attempting uninstall: optree\n",
            "    Found existing installation: optree 0.18.0\n",
            "    Uninstalling optree-0.18.0:\n",
            "      Successfully uninstalled optree-0.18.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: ml_dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.4\n",
            "    Uninstalling ml_dtypes-0.5.4:\n",
            "      Successfully uninstalled ml_dtypes-0.5.4\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 4.0.0\n",
            "    Uninstalling markdown-it-py-4.0.0:\n",
            "      Successfully uninstalled markdown-it-py-4.0.0\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.14.0\n",
            "    Uninstalling h5py-3.14.0:\n",
            "      Successfully uninstalled h5py-3.14.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.76.0\n",
            "    Uninstalling grpcio-1.76.0:\n",
            "      Successfully uninstalled grpcio-1.76.0\n",
            "  Attempting uninstall: google_pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: astunparse\n",
            "    Found existing installation: astunparse 1.6.3\n",
            "    Uninstalling astunparse-1.6.3:\n",
            "      Successfully uninstalled astunparse-1.6.3\n",
            "  Attempting uninstall: ai-edge-litert-nightly\n",
            "    Found existing installation: ai-edge-litert-nightly 2.2.0.dev20251223\n",
            "    Uninstalling ai-edge-litert-nightly-2.2.0.dev20251223:\n",
            "      Successfully uninstalled ai-edge-litert-nightly-2.2.0.dev20251223\n",
            "  Attempting uninstall: tb-nightly\n",
            "    Found existing installation: tb-nightly 2.20.0a20250717\n",
            "    Uninstalling tb-nightly-2.20.0a20250717:\n",
            "      Successfully uninstalled tb-nightly-2.20.0a20250717\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 14.2.0\n",
            "    Uninstalling rich-14.2.0:\n",
            "      Successfully uninstalled rich-14.2.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
            "  Attempting uninstall: kagglehub\n",
            "    Found existing installation: kagglehub 0.3.13\n",
            "    Uninstalling kagglehub-0.3.13:\n",
            "      Successfully uninstalled kagglehub-0.3.13\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.23\n",
            "    Uninstalling jaxlib-0.4.23:\n",
            "      Successfully uninstalled jaxlib-0.4.23\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.36.0\n",
            "    Uninstalling huggingface-hub-0.36.0:\n",
            "      Successfully uninstalled huggingface-hub-0.36.0\n",
            "  Attempting uninstall: ai-edge-quantizer-nightly\n",
            "    Found existing installation: ai-edge-quantizer-nightly 0.5.0.dev20251225\n",
            "    Uninstalling ai-edge-quantizer-nightly-0.5.0.dev20251225:\n",
            "      Successfully uninstalled ai-edge-quantizer-nightly-0.5.0.dev20251225\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.1\n",
            "    Uninstalling torch-2.9.1:\n",
            "      Successfully uninstalled torch-2.9.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: keras-nightly\n",
            "    Found existing installation: keras-nightly 3.14.0.dev2025122404\n",
            "    Uninstalling keras-nightly-3.14.0.dev2025122404:\n",
            "      Successfully uninstalled keras-nightly-3.14.0.dev2025122404\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.23\n",
            "    Uninstalling jax-0.4.23:\n",
            "      Successfully uninstalled jax-0.4.23\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "  Attempting uninstall: torch-xla2\n",
            "    Found existing installation: torch_xla2 0.0.1.dev202412041639\n",
            "    Uninstalling torch_xla2-0.0.1.dev202412041639:\n",
            "      Successfully uninstalled torch_xla2-0.0.1.dev202412041639\n",
            "  Attempting uninstall: tf-nightly\n",
            "    Found existing installation: tf_nightly 2.21.0.dev20251224\n",
            "    Uninstalling tf_nightly-2.21.0.dev20251224:\n",
            "      Successfully uninstalled tf_nightly-2.21.0.dev20251224\n",
            "  Attempting uninstall: ai-edge-torch-nightly\n",
            "    Found existing installation: ai-edge-torch-nightly 0.8.0.dev20251224\n",
            "    Uninstalling ai-edge-torch-nightly-0.8.0.dev20251224:\n",
            "      Successfully uninstalled ai-edge-torch-nightly-0.8.0.dev20251224\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, which is not installed.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "datasets 4.4.1 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2025.12.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "bigframes 2.31.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.2 which is incompatible.\n",
            "gradio 5.50.0 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.3 absl-py-2.3.1 ai-edge-litert-nightly-2.2.0.dev20251223 ai-edge-quantizer-nightly-0.5.0.dev20251225 ai-edge-torch-nightly-0.8.0.dev20251224 astunparse-1.6.3 backports.strenum-1.2.8 certifi-2025.11.12 charset_normalizer-3.4.4 filelock-3.20.1 flatbuffers-25.12.19 fsspec-2025.12.0 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.14.0 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 immutabledict-4.2.2 iniconfig-2.3.0 jax-0.8.2 jaxlib-0.8.2 jinja2-3.1.6 kagglehub-0.3.13 keras-nightly-3.14.0.dev2025122404 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.4 mpmath-1.3.0 multipledispatch-1.0.0 namex-0.1.0 networkx-3.6.1 numpy-2.4.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 opt_einsum-3.4.0 optree-0.18.0 packaging-25.0 pillow-12.0.0 pluggy-1.6.0 protobuf-6.33.2 pygments-2.19.2 pytest-9.0.2 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 rich-14.2.0 safetensors-0.7.0 scipy-1.16.3 setuptools-80.9.0 six-1.17.0 sympy-1.14.0 tabulate-0.9.0 tb-nightly-2.20.0a20250717 tensorboard-data-server-0.7.2 termcolor-3.2.0 tf-nightly-2.21.0.dev20251224 tokenizers-0.22.1 torch-2.9.1 torch-xla2-0.0.1.dev202412041639 tqdm-4.67.1 transformers-4.57.3 triton-3.5.1 typing_extensions-4.15.0 urllib3-2.6.2 werkzeug-3.1.4 wheel-0.45.1 wrapt-2.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_distutils_hack",
                  "absl",
                  "astunparse",
                  "backports",
                  "certifi",
                  "filelock",
                  "flatbuffers",
                  "gast",
                  "google",
                  "h5py",
                  "huggingface_hub",
                  "idna",
                  "jaxlib",
                  "markupsafe",
                  "ml_dtypes",
                  "mpmath",
                  "numpy",
                  "opt_einsum",
                  "optree",
                  "packaging",
                  "regex",
                  "requests",
                  "safetensors",
                  "scipy",
                  "six",
                  "sympy",
                  "termcolor",
                  "tokenizers",
                  "torch",
                  "torchgen",
                  "tqdm",
                  "transformers",
                  "triton",
                  "typing_extensions",
                  "urllib3",
                  "wrapt"
                ]
              },
              "id": "03957baf6e404ce58218cf8ddf84cbf6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ai-edge-litert-nightly in /usr/local/lib/python3.12/dist-packages (2.2.0.dev20251223)\n",
            "Requirement already satisfied: backports.strenum in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert-nightly) (1.2.8)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert-nightly) (25.12.19)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert-nightly) (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert-nightly) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert-nightly) (4.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert-nightly) (6.33.2)\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install ai-edge-torch-nightly --force-reinstall\n",
        "!pip install ai-edge-litert-nightly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DimeKTEw7rbE"
      },
      "source": [
        "### Build the .litertlm from the fine-tuned model\n",
        "\n",
        "After running the script below, you can find the converted model in located in `/content/litertlm/mobile-actions_q8_ekv1024.litertlm` in the colab environment. Copy it to a persistent storage like Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B1Sl__hlUWNP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from ai_edge_torch.generative.examples.gemma3 import gemma3\n",
        "from ai_edge_torch.generative.utilities import converter\n",
        "from ai_edge_torch.generative.utilities.export_config import ExportConfig\n",
        "from ai_edge_torch.generative.layers import kv_cache\n",
        "\n",
        "# Metadata for FunctionGemma\n",
        "llm_metadata = r\"\"\"start_token: {\n",
        "    token_ids: {\n",
        "        ids: [ 2 ]\n",
        "    }\n",
        "}\n",
        "stop_tokens: {\n",
        "    token_str: \"<end_of_turn>\"\n",
        "}\n",
        "stop_tokens: {\n",
        "    token_str: \"<start_function_response>\"\n",
        "}\n",
        "llm_model_type: {\n",
        "    function_gemma: {}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "checkpoint_dir = \"/content/mobile-actions-functiongemma\"\n",
        "\n",
        "litertlm_output_dir = '/content/litertlm'\n",
        "os.makedirs(litertlm_output_dir, exist_ok=True)\n",
        "\n",
        "# Create the LLM metadata file\n",
        "metadata_path = os.path.join(litertlm_output_dir, 'base_llm_metadata.textproto')\n",
        "with open(metadata_path, 'w') as f:\n",
        "  f.write(llm_metadata)\n",
        "\n",
        "# Import the weights and build the PyTorch model\n",
        "pytorch_model = gemma3.build_model_270m(checkpoint_dir)\n",
        "\n",
        "# Setup the export configurations and parameters for text generation models.\n",
        "export_config = ExportConfig()\n",
        "export_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED\n",
        "export_config.mask_as_input = True\n",
        "\n",
        "# Convert to LiteRT-LM Format\n",
        "converter.convert_to_litert(\n",
        "    pytorch_model,\n",
        "    output_path=litertlm_output_dir,\n",
        "    output_name_prefix=\"mobile-actions\",\n",
        "    prefill_seq_len=256,\n",
        "    kv_cache_max_len=1024,\n",
        "    quantize=\"dynamic_int8\",\n",
        "    export_config=export_config,\n",
        "    tokenizer_model_path=os.path.join(checkpoint_dir, 'tokenizer.model'),\n",
        "    base_llm_metadata_path=metadata_path,\n",
        "    output_format=\"litertlm\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix JAXlib dependency for .litertlm conversion\n",
        "!pip uninstall jax jaxlib -y -q\n",
        "!pip install jaxlib==0.4.23 jax[cpu]==0.4.23 -q"
      ],
      "metadata": {
        "id": "jSjwUpgnchts",
        "outputId": "71e43a2b-03e1-488c-ef33-440fd2f2a053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "chex 0.1.90 requires jax>=0.4.27, but you have jax 0.4.23 which is incompatible.\n",
            "chex 0.1.90 requires jaxlib>=0.4.27, but you have jaxlib 0.4.23 which is incompatible.\n",
            "orbax-checkpoint 0.11.31 requires jax>=0.6.0, but you have jax 0.4.23 which is incompatible.\n",
            "flax 0.10.7 requires jax>=0.6.0, but you have jax 0.4.23 which is incompatible.\n",
            "optax 0.2.6 requires jax>=0.5.3, but you have jax 0.4.23 which is incompatible.\n",
            "optax 0.2.6 requires jaxlib>=0.5.3, but you have jaxlib 0.4.23 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtnMtgeLbDsJ"
      },
      "source": [
        "### Save the `.litertlm` on Google Drive\n",
        "\n",
        "To deploy the converted model on [Google AI Edge Gallery](https://play.google.com/store/apps/details?id=com.google.ai.edge.gallery), we can first store the model on Google Drive. On the Gallery app, import the model from Google Drive later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "otpmb9ZkZPpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d6f4e7-77d2-4cb8-e63a-32ce3ba3955e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Mounting Google Drive on the colab environment\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OIa4FHNebNFb"
      },
      "outputs": [],
      "source": [
        "#@title Save the `.litertlm` on Google Drive\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/mobile-actions/\n",
        "!cp /content/litertlm/mobile-actions_q8_ekv1024.litertlm /content/drive/MyDrive/mobile-actions/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8ff452"
      },
      "source": [
        "## Summary and next steps\n",
        "\n",
        "Congratulations! You have completed the first part of [Fine-tune FunctionGemma 270M for Mobile Actions](https://ai.google.dev/gemma/docs/mobile-actions).\n",
        "\n",
        "You have successfully finetuned the FunctionGemma 270M with the Mobile Actions dataset and converted it to `.litertlm` format."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[FunctionGemma]Finetune_FunctionGemma_270M_for_Mobile_Actions_with_Hugging_Face.ipynb",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "677eb49a346f476ca415e70656d639cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b55d461723d4c0f9e1ba6f0c48bd1b0",
              "IPY_MODEL_1d6ad1f5f6bc4eddbe0eedec3853904c",
              "IPY_MODEL_1b8ed666faf44348a8cedb7e2378ec58"
            ],
            "layout": "IPY_MODEL_f4a978205df3449493eb25e1c7dc1e98"
          }
        },
        "7b55d461723d4c0f9e1ba6f0c48bd1b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_898e657f4eaf4d0d94b791ee9f72c8ff",
            "placeholder": "",
            "style": "IPY_MODEL_9f09181738504e9994f6b4cc9e635a00",
            "value": "Map:90%"
          }
        },
        "1d6ad1f5f6bc4eddbe0eedec3853904c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_001b438a8d0c487fa52e1d02cc39bc73",
            "max": 9654,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1838b294057946cfb4737abb8aef8351",
            "value": 8653
          }
        },
        "1b8ed666faf44348a8cedb7e2378ec58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57ba86da0fd94424b29f7b68ab57b1f0",
            "placeholder": "",
            "style": "IPY_MODEL_580f039a15644295a8dd5c410ebb4d59",
            "value": "8653/9654[00:09&lt;00:01,908.33examples/s]"
          }
        },
        "f4a978205df3449493eb25e1c7dc1e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "898e657f4eaf4d0d94b791ee9f72c8ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f09181738504e9994f6b4cc9e635a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "001b438a8d0c487fa52e1d02cc39bc73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1838b294057946cfb4737abb8aef8351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57ba86da0fd94424b29f7b68ab57b1f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "580f039a15644295a8dd5c410ebb4d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}